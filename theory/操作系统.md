

[TOC]



# 操作系统实战



## Hello

### 程序的运行

```c
#include "stdio.h"
int main(int argc, char const *argv[])
{
  printf("Hello World!\n");
  return 0;
}
```

运行过程：

1. C语言编译器 —> 具体硬件平台的二进制代码
2.  操作系统建立进程 并将二进制文件装入进程的内存空间 运行



**程序编译过程**

```shell
gcc HelloWorld.c -o HelloWorld
```

![](https://static001.geekbang.org/resource/image/f2/4a/f2b10135ed52436888a793327e4d5a4a.jpg)

具体执行的流程：

- 预处理：加入头文件 替换宏

  ```shell
  gcc HelloWorld.c -E -o
  ```

- 编译：包含预处理 C -> 汇编程序

  ```shell
  gcc HelloWorld.c -S -c
  ```

- 汇编：包含预处理和编译 汇编程序 -> 可链接的二进制程序

  ```she
  gcc HelloWorld.c -c 
  ```

- 链接：包含以上操作 将可链接的二进制程序和其他的库链接在一起 形成可执行的程序文件

  ```shell
  gcc HelloWorld.c -o HelloWorld
  ```

  

**程序装载执行**



图灵机：无限长的纸袋，纸袋上有无限个格子，读头根据格子的信息进行相关的操作

![](https://static001.geekbang.org/resource/image/69/7d/6914497643dbb0aaefffc32b865dcf7d.png)



冯诺依曼体系结构：

计算机必须具有如下功能：

- 能把程序和数据装入计算机中
- 可以长期记住程序 数据的中间结果和最终结果
- 可以完成各种算术 逻辑运算和数据传送等数据加工
- 可以控制程序走向，根据指令控制机器的各部件协调操作
- 能够按照要求将处理的数据结果显示给用户



所以计算机由五大组件构成：

- 输入设备
- 存储器
- 运算器
- 控制器
- 输出设备

![](https://static001.geekbang.org/resource/image/bd/26/bde34df011c397yy42dc00fe6bd35226.jpg)



```assembly
# hello对应的汇编代码
# 地址 机器码 汇编码
000000000040051d <main>:
  40051d:	55                   	push   %rbp
  40051e:	48 89 e5             	mov    %rsp,%rbp
  400521:	bf d0 05 40 00       	mov    $0x4005d0,%edi
  400526:	e8 d5 fe ff ff       	callq  400400 <puts@plt>
  40052b:	b8 00 00 00 00       	mov    $0x0,%eax
  400530:	5d                   	pop    %rbp
  400531:	c3                   	retq
  400532:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
  400539:	00 00 00
  40053c:	0f 1f 40 00          	nopl   0x0(%rax)
```





**将程序对应的机器指令装入内存**

![](https://static001.geekbang.org/resource/image/5d/6e/5d4889e7bf20e670ee71cc9b6285c96e.jpg)





### 实现一个最简单的内核



开机启动时：

- 执行BIOS固件中的指令 BIOS存在ROM中 可以理解为持久化的内存

  检测和初始化CPU 内存 主板凭条

- 加载硬盘中的第一个扇区的数据到0x7c00 开始的内存空间

- 跳转到0x7c00处 执行指令



## 设计



### 内核结构与设计

**内核需要有哪些组件**：

内核是计算机资源的管理者

计算机资源：硬件 + 软件

硬件：

- 总线
- CPU：执行程序，内核将运行时的程序抽象为进程，所以是进程管理
- 内存：分配与释放内存
- 硬盘：内核将用户数据抽象为文件，文件系统
- IO设备：IO管理器
- 网卡：网络协议栈
- 显卡：图形系统

还需要不同硬件对应的驱动程序管理和控制硬件



**经典内核结构**

- 宏内核：

  将以上的所有组件的代码都放在一起，形成一个大的可执行文件，优点在于组件之间可以相互调用，性能高，缺点在于高度耦合，没有模块化、扩展性、移植性

- 微内核：

  内核只有一些必要的核心功能：最少需要提供接收消息和发送消息的API

  其他组件如进程管理 内存管理等都做成服务进程

  请求和返回都需要经过内核

  伸缩性 扩展性好，但是性能差



**隔离硬件的差异**

与硬件平台相关的代码都放在硬件相关层



我们的选择：

![](https://static001.geekbang.org/resource/image/6c/3c/6cf68bebe4f114f00f848d1d5679d33c.jpg)





### 业界的内核架构



**Linux内核**

宏内核架构 模块之间通过函数调用交互

![](https://static001.geekbang.org/resource/image/92/cb/92ec3d008c77bb66a148772d3c5ea9cb.png?wh=2160*1620)



5大重要组件

![](https://static001.geekbang.org/resource/image/97/c9/97e7e66f9dcbddb1294ef9b7552fbac9.jpg?wh=3046x1502)



**Darwin-XNU内核**

MacOS和IOS的核心

两个内核层 Mach + BSD

![](https://static001.geekbang.org/resource/image/5e/8d/5e9bd6dd86fba5482fab14b6b292aa8d.jpg?wh=4462*4410)



**Window NT内核**



![](https://static001.geekbang.org/resource/image/c5/c9/c547b6252736375fcdb1456e6dfaa3c9.jpg?wh=4268*4905)





## 硬件

### CPU工作模式

x86CPU工作模式：

- 实模式
- 保护模式
- 长模式



```C
// 实模式下可以运行
int main()
{
    int* addr = (int*)0;
    cli(); //关中断
    while(1)
    {
        *addr = 0;
        addr++;
    }
    return 0;
}
```



**实模式**

> 实：真实
>
> - 不区分指令的内容 直接执行指令的功能
> - 发往内存的地址是真实的

x86 CPU实模式下的寄存器(16bit)

![](https://static001.geekbang.org/resource/image/f8/f8/f837811192730cc9c152afbcccf4eff8.jpeg?wh=1309*694)



实模式下访问内存：

分段内存管理模式

段寄存器的值 << 4 + 另外的寄存器(IP/SP...) 



实模式中断：

中断：中止执行当前程序 跳转到另一个特定的地址上 执行那里的代码

- 硬件中断：中断控制器发中断号 cpu响应
- 软件中断：cpu执行了INT指令 后面跟着中断号

有了中断号后 去中断向量表中超出中断号对应的CS:IP 即可执行中断处理程序

![](https://static001.geekbang.org/resource/image/e8/57/e8876e8561b949b8af5d5237e48f8757.jpg?wh=2783*1810)



**保护模式**

支持32位的处理能力

需求：

- 需要对CPU指令区分执行
- 需要限制CPU可访问的内存地址



保护模式下的寄存器

![](https://static001.geekbang.org/resource/image/0f/2a/0f564d0aac8514245805eea31aa32c2a.jpeg?wh=1389*819)



保护模式特权级：区别CPU指令

> R0可以执行所有指令 可以访问低特权级的资源
>
> > R1
> >
> > > R2
> > >
> > > > R3



保护模式段描述符：保护内存 -> 转化为保护段

一个段描述符有 64 位 8 字节数据，里面包含了段基地址、段长度、段权限、段类型（可以是系统段、代码段、数据段）、段是否可读写，可执行

访问一个内存地址的时候 段寄存器中存放的不是内存基地址 而是指向段描述符的索引 先访问段描述符 再判断其中的段信息能不能访问成功



保护模式段选择子：段寄存器中的内容

![](https://static001.geekbang.org/resource/image/d0/a4/d08ec3163c80a5dd94e488a71588f8a4.jpg?wh=4565*1513)

CS和SS中RPL就是自己的CPL

> CPL: Current Privilege Level RPL:Request Privilege Level DPL:Descriptor Privilege Level

当CPL < DPL时才能访问



保护模式中断：

中断程序地址 -> 中断们描述符（也是加了权限检查） 



**长模式**

也有特权级和权限检查，但是将内存地址空间的保护交给了MMU

支持64位的处理能力

长模式下的寄存器

![](https://static001.geekbang.org/resource/image/cc/34/cce7aa5fe43552357bc51455cd86a734.jpg?wh=1391*822)





### 地址转换

多程序并发存在的问题：

- 程序之间怎么保证没有内存的冲突
- 怎么保证不互相读写各自的空间
- 并发程序多了之后内存容量问题

  

解决方案：每个程序独享一个0-N的地址空间，与具体的系统软件和硬件环境无关（解耦）



**虚拟地址**

由链接器产生



**物理地址**

内存只认物理地址，物理地址就是地址总线上的信号



**虚拟地址到物理地址的转换**

MMU：内存管理单元，软硬件结合，硬件执行转换过程，但是依赖于软件提供的地址转换表



最初设想：

![](https://static001.geekbang.org/resource/image/d5/99/d582ff647549b8yy986d90e697d33499.jpg)





分页模型：

![](https://static001.geekbang.org/resource/image/9b/d0/9b19677448ee973c4f3yya6b3af7b4d0.jpg)



**MMU**

输入：虚拟地址和地址转换关系表（页表）

输出：物理地址





虚拟地址分为多个段，MMU用虚拟地址中的多个段的不同值 在不同级别的页表中查找 最后查找到物理地址

![](https://static001.geekbang.org/resource/image/2d/yf/2df904c8ba75065e1491138d63820yyf.jpg)



例子：保护模式下的分页-4KB页

最后得的地址：物理页基地址 + 页内偏移

![](https://static001.geekbang.org/resource/image/00/f8/00b7f1ef4a1c4f6fc9e6b69109ae0bf8.jpg)



MMU也会地址转换失败





### Cache与内存



**程序局部性原理**：CPU大多数时间都是在执行相同的指令或者与此相邻的指令（因为程序中循环很多）



**内存**：逻辑上可以看成一个字节数组

![](https://static001.geekbang.org/resource/image/1b/ed/1b41056cce55b17a2366d7b9dd922aed.jpg)



内存吞吐量比CPU差，并且多核CPU访问内存又会造成总线争用 进一步降低数据吞吐量



**Cache**：局部性原理的应用

![](https://static001.geekbang.org/resource/image/47/56/474189597993406a01a2ae171b754756.jpg)

Cache硬件工作流程：

- 读取：

  CPU发出的地址由Cache的地址转换模块分为：组 行 行内偏移两个部分

  如果 组 行都命中了 则用行内偏移读取数据直接返回给CPU

  如果没有命中 则分配一个新行 并访问内存 从内存中加载数据并放数据到新行中

  （如果没有新行了 则需要LRU替换行或者替换未修改过的Cache行 使得替换的成本最小）

- 写入：

  - 回写：写到Cache就返回
  - 直通写：写入Cache行的同时写入内存



Cache带来的一致性问题：

- 一个CPU核心中指令Cache和数据Cache的一致性问题

  数据Cache中的数据与指令Cache中的数据不一致

  对的部分将其内容写到内存 然后另外一个缓存从内存中重新加载

- 多个CPU核心间2级Cache的一致性问题

  相同的数据可能在每个CPU的2级缓存中都有一份 如果其中一个CPU修改了该数据 则其他CPU2级缓存就得失效

  多核心数据同步协议：MESI（硬件维护）

![](https://static001.geekbang.org/resource/image/97/bd/976f5cf91bc656e2a876235a5d2efabd.jpg)





## 同步原语

### 锁



并发操作中解决数据同步的四种方法：

- 原子变量
- 关中断
- 信号量
- 自旋锁



```c

int a = 0;
void interrupt_handle()
{
    a++;
}
void thread_func()
{
    a++;
}

```

![](https://static001.geekbang.org/resource/image/79/4c/79bfa1d036ebb27yy17ae3edf768ba4c.jpeg?wh=1920*1080)

t2时刻发生了中断



**原子操作 拿下单体变量**

解决并发问题：

- 将`a++`变为原子操作
- 控制中断 先关中断 执行完后开中断

加上lock前缀表示**锁定总线**

```c

//定义一个原子类型
typedef struct s_ATOMIC{
    volatile s32_t a_count; //在变量前加上volatile，是为了禁止编译器优化，使其每次都从内存中加载变量
}atomic_t;

// 原子加
static inline void atomic_add(int i, atomic_t *v)
{
        __asm__ __volatile__("lock;" "addl %1,%0"
                     : "+m" (v->a_count)
                     : "ir" (i));
}
//"lock;" "addl %1,%0" 是汇编指令部分，%1,%0是占位符，它表示输出、输入列表中变量或表态式，占位符的数字从输出部分开始依次增加，这些变量或者表态式会被GCC处理成寄存器、内存、立即数放在指令中。 
//: "+m" (v->a_count) 是输出列表部分，“+m”表示(v->a_count)和内存地址关联
//: "ir" (i) 是输入列表部分，“ir” 表示i是和立即数或者寄存器关联

```







# 操作系统理论

## 硬件结构

**CPU缓存**

![image-20220329170308513](https://gitee.com/yang_siping/static/raw/master/image-20220329170308513.png)

CPU从L1 Cache读取数据的速度是从内容读取速度的100多倍



CPU Cache的数据结构与读取过程

cache从内存中读取数据的单位是一个cache line 64B

CPU读取数据时 都会先访问cache 只有当cache中找不到数据时 才回去访问内存 将内存中的数据放入cache 再从cache中读取数据



访问：索引（内存地址取余） + 有效位 + 组标记

cpu每次都是读取一个字(Word)



提高缓存命中率：

- 提高数据缓存命中率：

  利用好局部性原理 最好访问内存中连续分布的数据 如数组 

  可以把线程固定绑到一个核心上

- 提高指令缓存命中率：CPU分支预测  如果根据历史预测到接下来要执行指令 if/else 就能提前把指令放在指令缓存中

  ```java
  Arrays.sort(nums);
  for (int num : nums) {
    	if (num < 50) {
        	do sth
      }
  }
  ```













## 系统启动

**操作系统核心轮廓**

- 多进程视图

  1. 计算机是以存储程序为思想作为基本结构，基本工作模式是：存储程序 - 加载程序 - 执行程序
  2. CPU需要在多个程序之间切换来保持忙碌
  3. 执行中的程序 -> 进程
  4. CPU在多个进程之间切换

- 文件视图

  将计算机中所有外设都统一抽象成文件

  如：显示器 磁盘 程序 图像 声音



**操作系统启动过程**

计算机工作的基本机理：取指 -> 执行 -> 取指 -> 执行



## 系统接口

> 通向操作系统内核



```c
// ls 核心代码
// 文件在磁盘上 显示器是硬件 open read printf就是操作系统提供给上层使用的系统接口
main()
{
  char buf[512];
  // open 打开文件
  fd = open(".");
  // read读取文件
  while (read(fd, buf, 512) > 0) 
  {
    // printf输出到显示器
    printf("%s", buf)
  }
}
```



```c
// shell核心代码
main()
{
  char cmd[100];
  while (1) 
  {
    scanf("%s", cmd);
    if (!fork())
    {
      execvp(cmd, NULL);
    }
    wait();
  }
}
```



**系统调用**

与进程有关的4个系统调用

- fork

  会创建一个进程为当前进程的子进程

  父进程中返回子进程的ID

  子进程中返回0

  ```C
  int main()
  {
    int pid;
    if (!(pid = fork())) 
    {
      printf("child: %d", pid);
      exit();
    }
    printf("parent: %d", pid);
  }
  ```

  

- exec

  进程就像一个壳子 exec就是用磁盘中的一个可执行程序替换了壳子中的内容

- wait 等待子进程的exit exit会给父进程发一个信号

- exit 终止一个进程



> 文件是用户操作计算机的基本单位



**系统调用的实现机理**

系统调用的作用：

- 给上层用户提供统一接口
- 上层系统只有通过系统调用才能进入操作系统，从而保护操作系统
- 执行在用户态的代码不能进入内核态区域



用户态：应用程序代码执行的状态

内核态：操作系统代码执行时的状态

> 代码都是装入内存后执行的 所以用户态代码和内核态代码在内存中的区域不同



![image-20211207125429262](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211207125429262.png)



特权级检查：用户程序执行的时候 每访问一次内存 都要做一次特权级检查 如果当前内存区域的特权级值 >要访问的内存区域的特权级值 则无法访问 



当且仅当: CPL <= DPL时 才能访问内存

![image-20211207130123583](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211207130123583.png)



**系统调用与int 0x80**

操作系统初始化GDT（全局描述符表）的时候 会将内核区域的DPL = 0

应用程序区域 CPL = 3



系统调用: 就是0x80号中断 其对应的中断处理程序的DPL = 3，CPL = 0

所以可以通过系统调用访问内核的其他区域





## 多进程

> 操作系统最核心的视图



**如何使用CPU**

朴素的想法：

1. 内存中分区一块区域

2. 调用磁盘驱动和文件系统 将一个可执行文件读入到分配好的内存区域中

3. CS:EIP = 这块内存区域的首地址

   取值 -> 执行 

   pc++

   取值 -> 执行..

4. 完成后 再重复1 执行另外一个程序



问题：当遇到IO指令时 CPU要停止工作 等待IO完成后才能继续工作 导致CPU利用率不高（等待IO完成的时间 CPU一直空闲）

IO指令：读写磁盘 屏幕输出 网络传输



![image-20211207174654945](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211207174654945.png)

并发概念的引出：当CPU执行到程序A的操作时 切换到程序B去执行 当程序A IO完成后 再切换到A执行



**进程概念与多进程视图**

进程切换：只切换PC指针 会导致寄存器中的值在切换回来时被污染 所以需要一个数据结构保存程序执行的上下文信息



进程：程序 + 记录当前程序的执行情况的数据结构（PCB）

程序：静态的指令和数据

进程：执行起来的程序



![image-20211207175649998](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211207175649998.png)





**多进程引起的基本问题**

> 并发：多个进程之间的来回切换
>
> 问题：
>
> - 什么时候切换 
>
>   CPU空闲的时候：
>
>   - IO
>   - exit
>   - 时间片用完
>
> - 怎么切换
>
>   1. 修改进程状态 PCB中的值
>   2. 将PCB挂到相关队列
>   3. 保存CPU寄存器的值到PCB
>   4. 从就绪队列中选出下一个进程的PCB
>   5. 将下一个PCB中的值还原到CPU寄存器中

操作系统中是通过PCB来管理进程的

PCB之间的关系也是线性关系 放在各种等待队列中

![image-20211207180318971](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211207180318971.png)



进程的生命周期

![image-20211207180429663](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211207180429663.png)



进程不能读写其他进程的内存空间：

如果用真实地址 则用户态内存区域的CPL = DPL = 3 则会存在进程相互修改内存区域的风险

所以需要引入虚拟内存 + GDT表 页表 来翻译CS:EIP



进程的通信：

- 读写同一个数据库
- 读写同一个文件
- 读写一段共享内存
- 读写一段内核态内存



## 线程切换与调度

进程切换：

- 资源切换

  内存

  文件

- 指令流切换

  CPU切换 也就是线程切换



**线程与进程**

线程概念的引入：

同一个可执行文件内 两个函数（指令执行流）也可以交替执行

进程概念的引入是为了并发执行多道程序 提高CPU利用率 

线程概念的引入是为了并发执行同一个可执行文件内的多个指令执行流 提高CPU利用率



![image-20211208130209383](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211208130209383.png)



**用户级线程的切换与创建**

用户级线程切换时：

1. 找到下一个线程的TCB （因为TCB中存了esp 通过esp就能找到栈中存放的执行现场）
2. 保存esp到TCB 将通用寄存器存到栈中
3. 下一个TCB中的esp恢复到CPU中
4. 遇到yield()函数的`}`时 从新的栈中弹栈 恢复通用寄存器 如ip





用户级线程的创建：

跟切换一个道理

![image-20211208180332426](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211208180332426.png)



函数调用传参：

bp = sp: 形成一个新的栈帧 [bp, sp]

bp + 4：存的是函数返回地址

bp + 8: 存的是第一个参数





**内核级线程的切换与创建**

用户级线程的问题：一个阻塞住了 由于用户级线程的TCB都在用户态 内核态感受不到 所以无法切换TCB 该进程的所有用户级线程都会阻塞住

这就限制了用户级线程的并发程度



内核级线程能被操作系统感知 可以分配到多个核上运行

而且同一个进程的多个内核级线程相比于多个进程 可以共享MMU和一些地址映射的缓存



进程 用户级线程 内核级线程的关系

- 引入进程是为了管理CPU 三者本质上都是指令执行序列
- 进程：要执行一个指令序列 要分配栈 创建数据结构记录执行状态 分配内存
- 内核级线程：将进程中的资源与执行序列分离 执行序列就是内核级线程
- 用户级线程：上层应用操控的多个执行序列 操作系统无法感知



内核级线程的切换

1. 切换TCB（得中断进入到内核中进行切换）获取内核栈的esp
2. 切换内核栈 内核栈中有用户栈的esp和pc等用户态执行现场信息
3. 切换用户栈
4. 用户程序PC指针切换



**CPU调度**

从线程的就绪队列中选择出一个线程 分配CPU执行

不同场景 CPU调度策略不同 PC机下的调度的基本准则

- 周转时间
- 响应时间
- 吞吐量



cpu调度算法：

- 先来先服务

- 短作业优先（只存在理论中）

- 最短剩余时间优先（实践中可以使用）

  适合非交互任务

- 时间片轮转（一段时间内 所有的任务都有机会向前执行 可以保证响应时间的上界：任务个数 * 时间片长度）

  适合交互任务

- 多级队列（操作系统中即存在前台任务 又存在后台任务）

  ![image-20211213174529442](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211213174529442.png)

- 多级反馈队列

  > 多级队列中存在的问题：
  >
  > 如果一直有前台任务 则后台任务得不到执行（饥饿）
  >
  > 如果后台任务得到了执行 则得将该任务执行完 前台任务才能得到相应（用户体验差）
  >
  > 后台任务执行剩余的时间不好预测

  ![image-20211213175304715](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211213175304715.png)

核心是时间片轮转，动态调整任务的优先级，首先分配所有任务一个较小的时间片，如果任务在第一个队列中没有执行完成，则说明该任务执行时间较长，则增大时间片，放到下一个队列中进行执行，这样交互任务能得到尽快的响应，后台任务又不会饥饿



## 进程同步

> 进程并发时 会相互依赖 所以操作系统需要提供一种机制来帮助进程实现这些依赖关系



**进程同步问题与睡眠 唤醒**

进程同步就是通过对进程的等待和唤醒的控制来使多个进程合理有序地向前推进

![image-20211215180249989](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211215180249989.png)



**信号到信号量**

> 如何实现进程间的等待和通知 -> 发信号



![image-20211215181227163](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211215181227163.png)

empty为信号 信号存在的问题：

当有p1 p2两个进程阻塞住的时候

c1 进程消费第一次的时候 唤醒一个p进程

但是消费第二次的时候 唤醒不了第二个进程了

问题：无法反映在信号上等待的进程个数，所以信号要与一个表示阻塞在信号上的进程个数的整数绑定，



信号量：用信号上关联的一个整数 来控制进程的阻塞与唤醒



```c
struct {
  // 信号量数值
  int value;
  // 等待在该信号量上的进程
  PCB* queue;
}

P(semaphore s) {
  	s.value--;
  	if (s.value < 0) {
      sleep_on(s.queue);
    }
}
V(semaphore s) {
  s.value++;
  if (s.value >= 0) {
     wake_up(s.queue);
  }
}
```



![image-20211216173205383](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211216173205383.png)





**临界区 对信号量的保护**

信号量数值的修改存在并发问题，需要被保护（对信号量的修改变成临界区代码，一次最多只有一个进程进入）

需要串行化，每个进程对信号量的修改要么一点都不做，要么全部做完，中途不能被打断，对信号量的修改必须是一个原子操作



> p1 p2进程里面都有`empty--`这段代码

![image-20211216173903925](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211216173903925.png)



软件实现：轮换 + 标记



轮换法：

![image-20211216175746285](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211216175746285.png)



缺陷：只保证了互斥性，p0退出后 如果p1没有得到调度 p0就无法进入临界区 不满足空闲让进



![image-20211216185735604](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211216185735604.png)

标记法：[0] = true 且 [1] = true时 两个都不能进临界区 不满足空闲让进



临界区的实现需要考虑：

- 互斥进入
- 空闲让进
- 有限等待





轮换 + 标记的结合：Peterson算法

![image-20211216192913123](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211216192913123.png)



临界区的硬件实现：

单CPU环境下 关中断(关中断后 无法进入内核 那也就无法进行进程切换 schedule函数)

关中断后 这个CPU再执行完每条指令后 不会检查并处理中断



TSL硬件指令 多个CPU共享内存 所以对于一个lock变量 可以保证只有一个进程返回false

![image-20211220172440448](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211220172440448.png)





**信号量的实现与使用**

信号量：内核中的一个数据对象 对上层提供PV操作的接口

- 信号量是一个需要被多个进程共享的整数
- 根据信号量的值 可以让对应的进程阻塞或者唤醒 操作对象是进程PCB
- 要保证多进程修改整数时的正确性 可以用软件算法实现 也可以用硬件实现



**死锁现象及死锁处理**

死锁：多个进程在同步时 由于互相等待对方向前执行 而造成谁都无法向前执行的僵局



死锁预防：破坏死锁发生的必要条件

![image-20211220175726258](/Users/yangsiping/Library/Application Support/typora-user-images/image-20211220175726258.png)

车的例子：

- 一条道路可以同时被两个车使用
- 某个车可以从已经进入的道路退出去
- 车在申请下一个道路不成功时 不会占住当前道路不放
- 没有形成环形等待



必要条件：

- 互斥：资源不能被共享 一个资源只能被一个进程使用（互斥是很多资源本身的特性 如临界区）

- 不可剥夺：进程已获得的资源 在未使用完之前 不能强行剥夺（由程序本身决定 否则会造成程序执行错误）

- 请求与保持：一个进程在请求新的资源阻塞时 对已获得的资源保持不放

  不保持 在没有任何资源的情况下 一次性申请进程所需的所有资源 就不会存在请求并保持这种状态

- 循环等待：进程之间形成一种头尾相接的循环性资源等待条件

  将资源编号 每次都从小到大申请 要申请的编号 > 当前持有的编号 



死锁避免：可以让进程随便申请资源 但是要判断申请是否存在死锁风险 如果存在则拒绝申请

银行家算法：如果存在一个调度方案 可以使系统安全

问题：避免机制是保守的 它是假设资源在进程结束后才释放 但实际上资源是进程运行中一直 申请-释放-申请-释放





死锁检测：预防和避免都是限制资源的使用 

不限制资源的申请 检测到发生死锁后处理死锁







## 内存管理

场景：

如何执行`mov %eax, [300]`这条指令

1. 将指令放入内存（内存管理）
2. pc指针指向这条指令（进程管理）
3. 取指令到cpu的IR寄存器中（计组）
4. 解释指令（计组）
5. 执行指令 地址总线上发出地址300 从数据总线取回内容并执行（内存管理）





**内存使用与程序重定位**

`call 40` -> `call 1040`

程序重定位：可执行文件中的逻辑地址转化为物理地址

重定位时机：

- 编译时：可执行文件只能放到那个地方
- 载入时：程序不能换出换入
- 运行时：每执行一条指令时 MMU转化



地址空间的切换：重定位寄存器重新赋基值（进程的内存区域基值记录在PCB中）



**分段**

一个程序分为多个段（代码 数据 栈） 每个段分别载入内存

段表：记录每个段的内存基址



**内存分区**

对于段请求：

从当前的空闲内存区域中找出一个能满足请求的内存区域分配

会生成内存碎片（想想数组和链表 这个就相当于是申请数组 需要一段连续的存储空间）

解决思想：将内存离散化 分割为大小固定的片 给段分配内存时按片分（分页的思想）



**分页**

物理内存和申请的内存段都分成了大小相等的页



问题：页表项太多 导致每个进程的页表占的内存太多



解决：从页表中去掉没有实际使用的逻辑页（导致页表项不连续 ）



问题：逻辑页到物理页的查找效率变低



两次页表：引入的页目录项（都是连续的 可以利用随机访问特性 快速定位）

再引入块表（缓存）

![image-20220105191819896](/Users/yangsiping/Library/Application Support/typora-user-images/image-20220105191819896.png)



分页：

- 将物理内存分成页 并以页为单位进行内存分配 可以降低分段请求带来的空间浪费
- 分页后 需要页表来进行地址转换 逻辑页->物理页框
- 多级页表可以降低页表项连续存放带来的空间开销
- 快表可以降低多级页表造成的时间开销



**段页式内存管理与虚拟内存**

段：用户想用

页：物理内存想用



虚拟内存：结合了分段和分页



两个问题：

- 程序如何放到内存中
  1. 从虚拟内存中分割出分区 建立程序段与分区的映射关系 即段表
  2. 虚拟内存分割成页 将程序段中的各个页的内存放到物理页框中 并建立页表

- 放到内存中的指令如何执行

  `call 40`等于跳转到CS:40

  查段表取出基址 1000 + 40 虚拟地址 1040

  虚拟页号为10 

  查页表再拼接偏移40 得出物理地址

  有专门记录段表初始地址和页表初始地址的cpu寄存器

  



## 换入/换出

> 用磁盘和时间换进程的一个规整的虚拟内存

内存：店面

磁盘：店面后的仓库

虚拟内存：仓库中所有东西的清单



**规整的虚拟内存**

操作系统给每个进程都提供了一个4G的虚拟地址空间，进程可以访问这个空间中的任何一个地址



![image-20220109174313869](/Users/yangsiping/Library/Application Support/typora-user-images/image-20220109174313869.png)



结合段页式内存管理

1. CS IP 结合段表 获取虚拟地址
2. 虚拟地址 结合页表 获取物理地址 定位到一个页表项
3. 当发现页表项中有效位为0 时 MMU向CPU发起缺页中断
4. 中断处理中 操作系统会去磁盘中找到这个虚拟页的内容并 分配一块空闲的内存页 读取到内存中这块内存页中 同时建立映射（填写页表项内容）
5. 中断处理返回后 会重新执行指令 再查此时页表项有效位为1

![image-20220110171743323](/Users/yangsiping/Library/Application Support/typora-user-images/image-20220110171743323.png)



**页面换出**

在找不到空闲物理内存页框时 需要换出一个页



页面换出算法：

- FIFO：选择最早换入的

- OPT：选择未来最远被访问的

- LRU：选择历史上最近最长时间没被访问的页面

  精确实现：

  - 基于时间戳：要存放在页表项中 每个虚拟页都关联一个时间戳 导致页表项的值增大 且时间戳也会溢出（有的机器一直跑）
  - 基于页面栈 维护一个链表数据结构 每次访问都更新指针 使得最近访问的页面在最上面

- clock：LRU算法的近似，硬件在访问页面的同时 自动更新页表项中的值 1表示被访问了 0表示没有（对最少的模拟）

  遇1变为0 遇0直接换出 相当于给最近被访问的页多一次机会

  优化：加一个扫描指针 定期将该进程的所有页框的访问位设置为0 这样对最近的模拟就是 0表示自上次扫描以来没有被访问过的





## 设备驱动

外设工作模式：

1. cpu `out ax 端口号` 通过端口地址发送对外设的工作要求
2. 外设工作完成后 通过中断机制告诉cpu cpu在中断处理程序中处理结果



**设备驱动的基本原理**

1. 发出命令
2. 中断处理



文件是对外设的抽象，操作系统将`write`操作转换为`out`语句

![image-20220117191424469](/Users/yangsiping/Library/Application Support/typora-user-images/image-20220117191424469.png)



**显示器的驱动**

1. open：去磁盘找到文件对应的FCB 读入内存中 将其与当前进程的PCB联系起来
2. 写到一个队列中，如果队列已满就睡眠等待 没有满就从用户态内存逐个取出字符
3. 输出完毕或者队列已满 从队列中取出字符写到显示器中（即光标所在位置）



**键盘的驱动**

1. 键盘中断 执行0x21号中断处理程序
2. 从0x60端口读取扫描码到
3. 将扫描码转化为ASCII码
4. 将ASCII码放入到读队列中
5. 再放到一个队列中  同时唤醒在这个队列上等待的进程（用户在 调用read方法的时候 会在这个队列上等待 sleep_if_empty）





## 文件系统

对磁盘的驱动 有五层抽象

- 从扇区到磁盘块
- 从请求单个磁盘块到磁盘请求队列
- 从磁盘请求到高速缓存
- 从盘块集合到文件
- 从多个文件到文件系统



**磁盘工作的基本原理**

1. `out ax 端口号` 告诉磁盘执行的工作 之后开始阻塞
2. 磁盘执行工作完成后 通过磁盘中断告诉CPU CPU执行中断处理程序 如将磁盘读入内容 复制到用户态内存buffer 并唤醒等待进程



![image-20220121185448406](/Users/yangsiping/Library/Application Support/typora-user-images/image-20220121185448406.png)

> 柱面
>
> >  磁道
> >
> > > 扇区



**磁盘读写的过程**：

1. 磁头移动（耗时最长） 找到柱面 不同盘面的磁头是一起移动的
2. 选择一个磁头上电 即选择了盘面 磁道
3. 旋转磁盘 将要读写的扇区移动到磁头的下方
4. 开始读写 将扇区的内容读到内存缓存区 或者将内存缓存区的内容写到扇区中(读写的过程 磁盘也在转动 磁头划过扇区)



磁盘读写时间：

- 移动磁臂 寻道 10ms
- 旋转磁盘 4ms
- 数据传输 0.01ms



**生磁盘的使用**

1. 从扇区到磁盘块请求的抽象

从给磁盘提供柱面号C 磁头H 扇区S 到提供一个扇区编号

因为数据传输时间最短 所以1号扇区设计到0号扇区的同一个磁道的后面的位置 这样就可以不用移动磁头或旋转磁盘

因为寻道+旋转一次读取k个扇区的策略 要比只读写一个扇区的速度提高近k倍

所以磁盘块就成为了磁盘读写的基本单位 磁盘块是连续的多个扇区

一般扇区大小为512B 磁盘块大小为4096B

（CPU缓存也是这个思想 所以一次读取一个缓存行大小的内存内容）

磁盘获取请求磁盘块的盘块号 -> 扇区号 -> C H S -> out指令



2. 多个进程产生的磁盘请求队列

最前面多加了一个步骤 即从磁盘请求队列中选择一个请求（请求数据结构中含有盘块号）

问题：如何从队列中选择一个请求（根据柱面号 因为寻道时间最长）

磁盘调度算法：需要减少总体的寻道距离 同时也要考虑公平性问题 避免造成饥饿

- 先来先服务
- 最短寻道时间：贪心
- scan： -> <-
- cscan：-> -> （实际采用的）



多进程访问磁盘步骤：

1. 进程提出磁盘访问请求：即将一个请求数据结构放到队列中

   同步保护这个队列

2. 磁盘中断处理时 从队列中获取一个磁盘请求进行处理（因为磁盘中断处理的发生 就是因为上一个磁盘请求处理完成引发的）

3. 读出数据结构中的盘块号 -> 扇区号 -> C H S -> out指令



3. 从磁盘请求到高速缓存

此时在用户眼中 读磁盘实际上是读高速缓存

读取磁盘数据时数据的流向：磁盘扇区 -> 内核态内存 -> 用户态内存

局部性原则：用户在读取一个磁盘块前100B的数据后 可能会再读后面紧挨着的100B 这时候磁盘块数据已经被读到内核态内存中了 不用再去磁盘中读了 -> 磁盘高速缓存

![image-20220121201930936](/Users/yangsiping/Library/Application Support/typora-user-images/image-20220121201930936.png)

所以要构建一个哈希表:

- key：盘块号
- value:高速缓存中的盘块

和一个空闲缓存块的链表：哈希表未命中 从空闲缓存块中取出一个块 从磁盘中读 并把数据放到缓存块中



**基于文件的磁盘使用**

4. 第四层抽象：引出文件

文件inode + 字符流读写位置 -> 物理盘块号

文件是一个连续的字符流

需要建立一个映射：字符流位置 -> 磁盘块号

文件对应的盘快号的存储方式：

- 顺序：FCB只需要存启示盘块号和总块数

  查找效率高但是 不方便修改（得移动元素）

- 链式：FCB中只需要存起始盘块号

  方便修改 但是查找效率低（得遍历链表）

- 索引存储结构：与跳表或LRU的思想一样 通过索引来快速定位一个链表结点

  就是现在的文件的FCB index node 索引节点 inode

  问题：

  - 文件过小 用索引块会不会很浪费空间（索引块大小同一个磁盘块） 
  - 文件过大时 一个索引块存储不下怎么办

  解决方案：

  分类处理：既提供直接索引（小文件） 又提供多级索引（大文件）



5. 第五层抽象：将整个磁盘抽象成一个文件系统

磁盘 -> 目录树

目录解析：根据文件路径获取文件的FCB

磁盘的目录的内容就是由一堆目录项构成的

目录项：文件路径字符串 + 对应的FCB地址（磁盘上所有文件的FCB构造一个数组 FCB地址就是这个数组的索引）



**操作系统使用磁盘的全过程**

1. 系统启动的时候 将磁盘的根目录文件innode(也就是PCB)读到内存中 作为1号进程的一个资源
2. 之后用户创建的任何一个进程都会继承这个FCB
3. 用户open文件的时候 会启动目录解析 最终找到目标文件的FCB 读入到内存中 返回文件句柄fd
4. 用户用fd操作文件 操作系统会根据pos和inode中存储的索引信息来找到pos对应物理盘块号block
5. 调用bread(block)读取磁盘高速缓存（内核态内存） 如果盘块内容已经在缓存中 则直接复制它并送回用户态缓存中 如果没有命中则获取一个空闲的高速缓存块（等待磁盘信息写入这个块中 这里只是先分配了）  
6. 发起磁盘读写请求 块号-> 扇区号 将请求放入电梯队列中 读进程等待
7. 当磁盘控制器处理完上一个请求后产生磁盘中断 然后从队列中取出这个请求 并将扇区号 -> C H S 
8. 磁盘控制器完成这次磁盘请求后会再次产生磁盘中断 中断处理程序中会唤醒睡眠的那个进程 此时磁盘数据已经从内核态内存的高速缓存中（就是之前分配的高速缓存块）复制到用户态缓存了 （用户可以读该盘块的内容了）





## 网络系统

**零拷贝**



DMA：由DMA控制器负责把数据从磁盘控制器缓冲区搬运到内核缓冲区 内核缓冲区到用户空间的搬运工作还是CPU做



实现本地磁盘文件对外传输的功能：

传统实现：

- 4次用户态与内核态的上下文切换

```c
// 一次系统调用会对应2次用户态与内核态的上下文切换 
// 切换原因：用户空间没有权限操作磁盘或者网卡这种外设 内核有权限
read(file, temp_buf, len);
write(socket, temp_buf, len);
```

- 4次数据拷贝
  1. DMA：磁盘控制器缓冲区 -> 内核缓冲区（PageCache 也会跟CPU缓存一样 有预读功能 局部性原理 这也是顺序读快的原因 想想数组和链表）
  2. CPU：内核缓冲区 -> 用户缓冲区
  3. CPU：用户缓冲区 -> Socket缓冲区（内核中）
  4. DMA：Socket缓冲区 -> 网卡缓冲区



优化思路：

- 减少上下文切换 -> 减少系统调用次数

- 内存拷贝的次数 -> 因为在用户空间中不会对数据进行修改



零拷贝的实现：

- mmap + write：可以减少一次数据拷贝

  mmap：将内核缓冲区的数据映射到用户空间

  ```C
  buf = mmap(file, len);
  write(sockfd, buf, len);
  ```

  ![](https://gitee.com/yang_siping/static/raw/master/20220327122753.png)

- sendfile

  所有数据拷贝都是DMA做的 CPU没有参与

  ![image-20220327123057023](https://gitee.com/yang_siping/static/raw/master/image-20220327123057023.png)



PageCache应用于读取大文件遇到的问题：

- 热点小文件无法充分使用PageCache磁盘读写性能下降 会污染PageCache
- 大文件还需要额外拷贝到PageCache一次才能到用户缓冲区



解决方案：

异步IO + 直接IO

- 用户进程发起读请求后就可以执行其他任务
- 内核将数据从磁盘控制器缓冲区拷贝到用户缓冲区 通知用户进程读取 不经过PageCache



直接IO:

使用场景：

- 应用已经实现了磁盘数据的缓存
- 大文件的读取

劣势：

- PageCache中会用IO调度算法合并IO请求 从而减少磁盘的寻址操作
- 无法利用PageCache预读



**IO多路复用**

> select / poll / epoll

最基本的Socket模型

注意：监听的socket和实际用于传输数据的socket是两个

![image-20220329115113389](https://gitee.com/yang_siping/static/raw/master/image-20220329115113389.png)



缺点：一个server只能服务一个client



多进程模型：

父进程负责accept  fork出来的子进程负责read write建立好的socket



多线程模型：

来一个连接就创建一个线程去处理socket 或者是线程池的方案



IO多路复用：

> 多路复用API返回的socket不能保证都是可读写的 所以最好搭配非阻塞IO
>
> 通过一个系统调用 来从内核中获取多个事件

一个进程管理多个socket 进程每次只需要处理产生了事件的socket 

进程先把所有连接（socket也是文件 所以传的是文件描述符） 传给内核 

内核会返回所有产生了事件的连接 进程在用户态再处理这些连接



select/poll

都是传一个线性表到内核中 内核遍历线性表 判断出socket可读或者可写的 再传回用户态 用户进程再遍历线性表 找出被判断为可读或者可写的socket 进行处理

select 是数组 poll是一个链表



epoll

- 用红黑树管理一个进程所有需要观测的socket 每次只需要传入一个待检测的socket 时间复杂度是O(logn)
- 维护了一个链表来记录有事件发生的socket 用户进程需要获取时先判断时间发生的socket个数是否不为0  如果不为0  则从内核空间返回socket链表



两种事件触发模式：

- 边缘触发：当红黑树中有事件发生时 只会通知一次服务器端 所以要保证一次性将内核缓冲区的数据处理完 效率比较高 因为可以减少epoll_wait系统调用的次数

- 水平触发：当红黑树中有事件发生 会不断通知服务器端 直到数据处理完

  是select poll epoll默认的触发模式





**网络模式：Reactor Proactor**

Reactor模式：非阻塞同步网络模式 感知的是就绪的可读写事件

对IO多路复用的一层封装 页脚Dispatcher模式 在IO多路复用监听到事件后 根据事件类型(连接 可读 可写)  分发给某个进程/线程



组成：Reactor（1个或者多个） + 处理资源池（一个或者多个 进程/线程）

- Reactor：监听和分发事件 事件类型：连接 读写
- 处理资源池：处理事件 read -> 业务逻辑 -> send



方案：

- 单Reactor 单进程/线程（资源池里面只有一个进程/线程）

  ![image-20220329142414061](https://gitee.com/yang_siping/static/raw/master/image-20220329142414061.png)

  Reactor -> Acceptor 处理连接事件 / Handler 处理读写事件

  优点：实现简单 无需考虑竞争问题

  缺点：无法发挥多核CPU的性能 如果handler对象在业务处理时耗时太长 就影响其他读写事件的处理

  适用场景：业务非常快速的情况  如Redis

- 单Reactor 多进程/多线程

  ![image-20220329143250810](https://gitee.com/yang_siping/static/raw/master/image-20220329143250810.png)

​	将业务处理交给线程池中的线程处理

​	优点：充分利用了多核CPU

​	缺点：需要额外处理线程之间的协作与互斥 单Reactor还是会成为瓶颈

- 多Reactor 多进程/线程

  ![image-20220329160218079](https://gitee.com/yang_siping/static/raw/master/image-20220329160218079.png)

​	主线程：负责接收新连接 并把新连接传递给子线程

​	子线程：获取连接后监听该连接的事件 有事件发生则自己处理



Proactor模式：异步网络模式 处理的是已完成的读写事件

阻塞IO：read后 线程阻塞直至数据到达用户空间

非阻塞IO：当数据还没到内核缓冲区的时候一直read轮询 到达了之后阻塞将数据从内核缓冲区搬运到用户空间（这个过程是同步的 因为需要CPU参与）

异步IO：aio_read后 立即返回 内核会自动将数据拷贝到用户空间 拷贝完成后通知数据









​	

​	







