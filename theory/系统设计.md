# 系统设计

[TOC]



## 新鲜事系统 

**面试形式与常见的面试题**

- 设计某某系统
- 设计某某系统中的某某功能



**评分标准**

- 可行解
- 特定问题
- 分析能力
- 权衡
- 知识储备



**4S分析法**

- Scenario 场景
  - 需要设计哪些功能 要设计得多牛
  - features qps DAU  interfaces
- Service 服务
  - 将大系统拆分为小服务
  - application module
- Storage 存储
  - 数据如何访问和存储
  - schema data SQL NoSQL File System
- Scale 升级
  - 解决系统的缺陷 可能遇到的问题
  - Sharding Optimize Special Case

> 前3个就已经构成了一个可行解 加上最后一个才是完美解

**系统 = 服务 + 存储**



**Scenario 场景**

首先需要问的

- 需要设计哪些功能：可能只是系统中的某些功能（也可以自己想）
  1. 罗列出该系统所需的功能 Twitter
     - register login
     - user profile display / edit
     - upload image / video
     - search
     - post tweet
     - timeline 
     - news feed
     - follow / unfollow user
  2. 选出核心功能
     - post tweet
     - timeline
     - news feed
     - follow / unfollow a user
     - register login
- 需要承受多大的访问量：
  - DAU
  - QPS 也可以从DAU推出来

> 分析QPS的作用
>
> QPS = 100 
>
> - 用笔记本做Web Server
>
> QPS = 1k
>
> - 好一点的Web Server
> - 需要考虑single point failure
>
> QPS = 1m
>
> - 需要一个1000台服务器的集群
> - 需要考虑maintainance （某一台挂了怎么办）
>
> QPS与 web server 和 database的关系
>
> - 一台Web Server 1k qps
> - sql databse: 1k qps
> - nosql database(cassandra) 10k qps
> - nosql database(memcached) 1m qps



**Service 服务**

将大系统拆分为小服务

1. 重新过一遍需求 将每个需要添加为一个服务
2. 归并相同的服务(服务是逻辑处理的整合 同类问题的处理归并在一个服务里面 单一职责原则)



Tweet:

- user service

  register

  login

- tweet service

  post tweet 

  news feed

  timeline

- medie service

  upload image/media

- friendship service

  follow

  unfollow



**Storage 存储**

数据如何访问与存储

1. 为每个Service选择存储结构
2. 细化表结构(挑关键的写）



存储：

- database

  SQL

  NoSQL

- File System

- Cache



**News Feed**

*Pull Model*

算法：

用户在查看自己的News feed的时候 获取每个好友前100条tweets 合并出前100条（K路归并算法）

复杂度分析：

- news feed: N个关注的对象 1DB read获取所有关注对象 N DB read获取所有的tweets + N路归并时间（在内存 所以可以忽略）
- post tweet：1 DB write

存在的问题:  pull 需要n + 1次DB read 



*Push model*

算法：

- 为每个用户建一个list存储他自己的news feed

- 当用户发一个tweet之后 将推文逐个推送到关注该用户的用户和用户自身的list中

  fanout

- 用户查看news feed的时候 只需要从news feed list中读出前100条即可

复杂度分析：

- news feed : 1DB read
- post  tweet ： N DB write N为粉丝数量（可以异步写）

问题：粉丝的数量可能很大 粉丝数量不大可以用push



**Scale 扩展**

系统如何优化与维护

优化：

- 解决设计缺陷

  pull vs push

  pull缺陷：读慢 -> DB read之前加缓存 cache timeline cache news feed

  push缺陷： 

  浪费更多的存储空间（不是大问题）

  不活跃用户

  粉丝数量多

- 更多功能设计

  like follow/unfollow 

- 特殊情况

  粉丝量巨大和流量巨大时

  僵尸粉

维护：

- 鲁棒性

  有一台服务器/数据库挂了怎么办

- 扩展性

  流量暴增 如何扩展





## 秒杀系统与订票系统设计

**Scenario 场景**

例子：

- 抢购
- 抢红包
- 抢火车票



QPS分析

日常QPS 1000 秒杀时QPS 100,000 增加100倍以上



需要解决的问题：

- 瞬时高并发
- 库存不能超卖
- 脚本请求
- 设定固定时间开启
- 严格限购



需求拆解：

- 商家侧：
  - 新建秒杀活动
  - 配置秒杀活动
- 用户侧：
  - 秒杀页面
  - 购买
  - 付款



**Service**

单体架构 vs 微服务架构

单体架构缺点：

- 前后端耦合 服务压力大（请求页面和购买商品都是请求同一个web server）
- 各功能模块耦合严重 不方便对某个模块进行单独升级 单独开发 开发技术和语言也必须一致
- 扩展性差 难以对某个模块单独扩展
- 数据库崩溃导致整个服务崩溃

微服务架构优点：

- 各功能模块解耦 保证单一职责 可单独升级 单独扩容 单独开发 单独选型
- 故障隔离



可分为：

- 秒杀服务
- 商品信息与库存服务
- 订单服务
- 支付服务



**Storage**

表结构：

- 商品信息表

- 秒杀活动表
- 库存表
- 订单表



超卖问题的解决：

1. 先判断库存 然后扣减库存（会导致超卖）

   ```mysql
   select stock from stock_info where commodity_id = 1 and seckill_id = 1;
   update stock_info set stock = stock - 1 where commodity_id = 1 and seckill_id = 1;
   ```

2. 加上事务和行锁（不会导致超卖）

   ```mysql
   begin;
   select stock from stock_info where commodity_id = 1 and seckill_id = 1 for update; # 加上了行锁
   update stock_info set stock = stock - 1 where commodity_id = 1 and seckill_id = 1;
   commit; # 提交事务 释放行锁
   ```

3. 使用update语句自带的行锁（不会导致超卖）

   ```mysql
   select stock from stock_info where commodity_id = 1 and seckill_id = 1;
   update stock_info set stock = stock - 1 where commodity_id = 1 and seckill_id = 1 and stock > 0;
   ```



瞬时高并发问题的解决：库存预热到Redis缓存中 Redis单机QPS 10W

**秒杀的实质 就是对库存的抢夺**

100W个秒杀请求都打到MySQL 导致MySQL挂掉

Redis支持Lua脚本实现CAS功能 从而实现扣减库存的原子性

加了Redis后只有Redis扣减成功的请求 才会到达MySQL 如果只有100台商品 那么到MySQL的请求就只有100条

问题：如果秒杀数量是1W台或者10W台 实际到达MySQL的请求还是巨大

方法：通过消息队列进行削峰 （当生产者和消费者的速率不一样时使用）



库存扣减时机问题：

- 下单时：会导致下单了但是不支付 其他人也无法购买
- 实际支付后：会导致客户下单了 但是支付失败
- 下单先锁库存 支付后再扣减：不会有上面的问题



限购问题：Redis set



付款和减库存的数据一致性：分布式事务

付款：

1. 支付服务：添加支付记录
2. 订单服务：修改订单状态
3. 商品服务：扣减商品库存



**Scale**

遗漏的功能：Redis库存扣减完后 抢购按钮可以取消

其他问题：

- 防止刷爆商品页面（前端静态资源的请求） CDN

- 计算倒计时：获取后端时间

- 秒杀服务器挂掉：尽量不要影响其他服务 特别时非秒杀商品的正常购买

  服务熔断（fail fast）

- 防止脚本：

  - 验证码机制
  - 限流机制：是否来自同一IP 同一用户
  - 黑名单机制：黑名单IP 黑名单用户

  

  

## 用户系统：数据库与缓存



**Scenario**

其实也就是考虑功能性需求和非功能性需求

要实现的功能：

- 注册
- 登录
- 用户信息查询(QPS最大)
- 用户信信息修改

QPS分析：QPS决定了服务的存储

100M DAU 一天中的秒 86400 ~ 10^5

信息查询：查看好友 发消息 更新主页

100M * 100(假设一天100次) / 10^5 = 100K

peak = 3 * 100k = 300k(因为用户使用的时间点都比较集中)



**Service**

- 注册 登录服务
- 用户信息存储与查询服务
- 好友关系服务



**Storage**

用户系统特点：读多写少 —> 用Cache进行优化

```java
// 1.因为有可能第一个成功 第二个失败
cache.delete(key);
database.set(key, user);
// 2.多线程下该方法仍然会导致数据不一致 看到旧数据
database.set(key, user);
cache.delete(key);
// 还是会存在问题：多进程问题 database成功 cache失败 写很多的情况下 没有优化效果 反而要频繁的更新cache的消耗
// 3. cache设置超时时间 允许短期不一致 但最终会一致（tradeoff）
```



两种Cache和DB组合的方式：

- Cache aside：服务器分别与DB和Cache进行沟通（比较多）
- Cache through：服务器只与Cache沟通 cache负责与DB沟通进行持久化



Friendship Service:

单项好友关系和双向好友关系：

friendship table

from_user_id to_user_id

单项存一份

双向存两份（因为要为这两个字段建立索引）



**SQL与NoSQL的选择**

- 大部分情况下都可以

- 需要事务支持要用SQL

- SQL：结构化数据（ORM） 自由创建索引

  NoSQL：分布式 auto-scale replica

- 一般可以组合

- user table 一般存在SQL：信任度 multi-index

- friendship：NoSQL 结构简单 key-value查询效率更高





## API设计与短网址系统

**RESTful API**

- 每个URL代表某种类型的一个或者多个数据
- POST DELETE GET PUT 代表对数据增删查改
- 条件和参数都放在HTTP的参数里



**Design News Feed API**

只需要考虑与前端进行交互的接口设计 不需要考虑后台数据的存储与获取

问题：

- 设计获取News Feeds List的API请求格式 
- API返回的内容格式
- 设计翻页
- Mentions的数据格式



**短网址系统**



**Scenario 场景**

需要确定的问题：

- long url与short url之间是否需要一一对应
- short url长时间没人使用需要释放吗



DAU -> QPS（生成短网址的和获取短网址的）



**Service 服务**

url service



**Storage **

key-value



短网址生成算法：

- 随机生成
- 顺序生成整型ID 整型ID再转化为字符



## 优惠券系统设计

**Scenario **

优惠券种类：

- 使用次数
- 使用时间限制
- 减价策略
- 先决条件
- 如何分发



优惠券核心流程：

- 发券：
  - 发送方式：同步发 异步发
  - 减价方式：绝对值 百分比
- 领券：
  - 谁能领：所有用户 指定的用户
  - 领取上限
  - 领取方式：主动领 自动发放
- 用券：
  - 作用范围：商品 商户 类目
  - 计算方式：是否互斥 是否达到门槛
  - 使用时间：活动期间
  - 使用次数：一次还是多次



功能性需求：

商家侧：

- 创建优惠券
- 发送优惠券

用户侧：

- 领取优惠券
- 下单
- 使用优惠券
- 支付



**Service**

生成系统与使用系统



设计难点：

- 分布式事务
- 防止超发
- 大批量给用户发券
- 限制券的使用条件
- 防止用户重复领券



**Storage**

表单：

- 券模板
- 券
- 规则



## 数据库拆分与一致性哈希算法

*how to scale system ~ how to scale database*



单点失效：当只有一台web server和database的时候会出现

解决方案：

- 数据拆分：sharding

  按照一定的规则 将数据拆分 分开存储到不同的实体机上

  作用：

  - 挂了一台不会全挂
  - 分摊读写流量

- 数据复制:   replica

  一式三份

  作用：

  - 一台机器挂了可以用其他两台的数据恢复
  - 分摊读请求



**Sharding**

- **Vertical Sharding**： 

  - user table 放一台 message table放一台
  - 一张表中的字段拆分

- **Horizontal sharding**: 一般是这个

  hash(key 一般是user_id) -> db_num

  使用一致性哈希算法目的：

  在改变数据库数目时使得数据迁移量尽可能少 且均匀分布到各个数据库上 使得服务器压力不会太大 数据不一致的时间不会太久

  

**Replica**

backup和replica的区别：

- backup:
  - 周期性执行
  - 数据丢失时 只能恢复到之前的某个时间点
  - 不用做在线的数据服务 不分摊读请求
- replica：
  - 实时执行 在数据写入的时候 会多写几份
  - 当数据丢失的时候 可以通过其他copy恢复
  - 可以用做在线等额数据服务 分摊读请求





### RateLimter设计

场景：一分钟内密码输入错误数不超过5次



**Scenario**

- 根据网络请求的特征进行限制

  某个IP/User 的行为

- 系统粒度

  某个时间段内超过了一定的数目 就拒绝该请求

  粒度不用太细



**Service**

无法细分



**Storage**

数据特征：

- 记录某个特征在某个时刻做了什么事
- 信息保留时间不用太长（比如5/m 那就只需要记录一分钟内的数据）
- 满足高效存取

选取的存储：Memcached 无需持久化



key：feature + timestamp + event

value：次数





### Monitor System设计

**Scenario**

- 对于某个链接的访问次数的曲线图
- 查询某个链接的总共的访问次数



**Service**

无法再细分



**Storage**

- 写多读少
- 需要持久化存储
- SQL NoSQL File System都可以

核心：分级存储 key：url value：所有访问记录的统计

- 今天数据：聚合粒度 分钟
- 昨天数据：聚合粒度：5分钟
- 上个月数据：聚合粒度：1个小时
- 去年数据：聚合粒度：周





## 分布式文件系统设计

原理：用多台机器去解决一台机器不能解决的问题（存储 QPS）

三驾马车：

- 分布式文件系统 GFS：存储数据 写少 读多的系统
- map reduce：处理数据
- bigtable: no-sql database 连接底层存储与上层数据



**Scenario** 需要设计哪些功能

需求：

- 用户写一个文件和读一个文件
- 支持的文件容量越大越好
- 多台机器存储这个文件
- 机器越多越好



**Service**

client - server

server内部两种结构：

- peer 2 peer

  优点：一台机器挂了还可以工作

  缺点：多台机器之间需要经常通信保持一致

- Master Slaver

  优点：

  - 设计简单
  - 数据很容易保持一致

  缺点：

  单master会挂

最终方案：

- Master + slave
- 挂了重启就行



**Storage**

单个操作系统怎么储存一个文件的：

多级页索引 一个页4KB



**如何存储大文件**：将文件按照更大的单位进行存储 1chunk = 64MB



**如何将一个大文件分别存储在多个机器上**：

master + slave

- master的作用：

  存储文件数据的metadata

  存储filename + chund index -> slave ID之间的映射关系(不存储具体的offset)

- slave存储具体的文件分片



**如何写入一个文件**：

- 一次：如果出错 得重传整个文件
- 多次：如果出错 只需要重传出错的那一份（按chunk来传）

写入步骤

1. client告知master 自己要写那个文件 写哪个chunk
2. master分配一个chunk server的地址
3. client 将文件chunk写到chunk server
4. chunk server返回信息给client和master



**需要修改文件怎么办**：

删掉 再重新写一份



**如何读一个文件**：

1. client 发送文件名给master
2. master返回一个chunk list
3. client再根据这个chunk list到各个chunk server上去请求chunk



**Scale**

> 系统如何维护和升级

单master 90%的系统都在使用 simple is perfect



**如何判定磁盘上的一个chunk是否损坏**

CheckSum(哈希算法)

- 写入时机：写入一个chunk的时候
- 检查时机：读取一个chunk的时候



**如何在chunk server down掉的时候 避免数据丢失**

*Replica*：做数据备份

备份策略：

- 三个备份

- 两个备份放在较近的位置 一个放在较远的位置



**当chunk损坏时怎么处理**

1. 向master询问 其他的chunk server地址
2. 根据master返回的地址 获取备份



**如何发现一个chunk server down掉了**

client 定时向master发送HeartBeat 



**如何解决replica时 client端需要写多份数据的压力**

丢给某个chunk server （选leader）让chunk server进行扩散



**怎么选chunk server的leader**

- 找最近的
- 找当前负载最小的



**如果在写入的时候 chunk server挂了怎么办**

重新写入





## BigTable设计

> Google内部  开源的为Hbase

**Scenario**

根据key返回一个value



**Storage**

将数据分为一个一个的块放在磁盘上 块内的数据是有序的（局部有序） 这样就可以支持二分查找 块大小为256M



修改：（写优化）

- 直接在文件里面修改：某个数据大小发生变化 需要移动数据 4B -> 8B 

- 读文件修改好后 将原来的文件删除 重新写入新文件 每次都要读入和写很不变的内容

- 不修改 直接append 修改操作 (a = 3) 在文件的最后 最快 

  带来的问题：

  - 怎么识别哪个是最新的记录：从后往前扫描 时间戳最大的就是真正的数据（类似于MVCC）

  - append打乱了有序性 怎么使用二分：

    - 每一个块都是内部有序
    - 写的时候 只有最后一块是无序的 并且隔一段时间整理为有序

  - 会有很多重复数据

    需要定期K路归并 合并相同数据（取时间戳最大的）



**完整读写过程**

**写入过程**：内存排序 + 1次硬盘写入数据 + 1次硬盘写Log（顺序写盘很快 相当于一次随机写盘）

内存：

- 写一次Write Ahead Log（顺序写盘）
- 将数据加入到跳表中

磁盘：

- SkipList满了 就统一写到硬盘中



 **读出过程**：

1. 内存：

   查跳表：最新一个块的数据

   查bloomfilter

   查询index（各个块对应的跳表的index）

2. 磁盘：

   去序列化后跳表中找到value



问题：在一个File中怎么查询一个Key对应的value

解决：磁盘二分 -> 建立内存索引(一个索引对应的范围是一个块)



问题：如何更快的检测一个Key是否在File里面 内存索引只是减少了磁盘扫描的范围 如果能指定key不在file里面 那就免去了一次磁盘IO

BloomFilter： 位图 + 哈希

一个key: 通过多个hashFunction （使用多个哈希 是为了减少哈希碰撞）表示成多个数 在位图（一个逻辑上的bit数组）上就是多个bit = 1 用这几个bit = 1来表示该key存在于集合中



减少哈希碰撞的方法：

- 增大bit数组长度
- 增加哈希函数个数
- 减少元素个数



误判：false positive 误判率在3%-4%左右

- 说key存在 那有可能存在（哈希碰撞）
- 说key不存在 那就一定不存在





**Scale**

之前是单机环境下的

怎么存储读取1PB的数据

Horizontal Sharding  将一张表的数据分到不同的机器上



如何管理server

- Master + slave
- Master管理 一致性哈希的map  key -> server address



集群环境下怎么读数据

1. client -> master key
2. master -> client key对应的server id
3. client再去对应的server 查询（此时为单机查询）



集群环境下怎么写数据：

1. client -> master key
2. master -> client key对应的server id
3. client再去对应的server 写入（此时为单机写入）



每台数据越写越多 Local disk存不下怎么办 或者某个数据丢失怎么办

将数据存到GFS中

优点：

- 磁盘容量是可扩充的
- 帮你做的replica
- 写失败和恢复都帮做了



如何避免多个client同时读写一个key

多台机器组成的分布式锁服务

跟master申请server id的时候 先去判断该key是否已经上锁





## 微信设计

**Scenario**

基本功能：

- 两个用户互相发消息
- 群聊
- 用户登录注册
- 通讯录

其他功能：

- 用户在线状态
- 限制多机登录



微信 一天发送量 450亿

平均QPS 450 10^8 / 10^5 = 450K

peak QPS 450K * 3 = 1.35m



**Service**

- Message Service：信息相关的存取
- Realtime Service：信息的实时推送



**Storage**

Message Table表设计

Thread Table 会话表 thread has a list of message



thread table 

user thread table：因为有的会话信息是私有的 如未读数 是否免打扰



发送时如何A如何找到与B的thread_id

thread table中加一个字段：participants_hash_code

code=hash(sort(participants_user_ids))



Message Table数据特点：

- 数据量大 不需要修改 就跟Log一样

存储结构：

- row_key sharding key：thread_id
- column key：created_at
- value 其他信息



thread table：存储公有的thread信息 SQL / NoSQL都可以

UserThread Table NoSQL



可行解的流程：

1. 用户A发送一条消息给用户B
2. 服务器收到消息后 查询是否有A和B的对话记录 如果没有则创建对应的thread
3. 根据ThreadId创建Message
4. B（上线后）每隔10s 轮询服务器获取最新的信息(poll)
5. B收到信息



**Scale**

- 更好的信息更新方式
- 其他小功能的设计



Socket使得服务端向客户端推送数据

1. 用户A打开app后 找Web Server要一个Push Service的连接地址（IP Port）
2. A通过socket与push server连接
3. 用户B发送消息给A 消息先被发送到服务器
4. 服务器把消息存储后 告诉Push server 让他通知A(如果push server down了 用户A可以感知到 那就fall back to poll)
5. A收到及时的消息提醒



**如何支持群聊**

群聊500人 web server会发送500次push请求到push server上 但实际上在线的人不多 web server又不知道有多少人在线



增加一个中转站：Channel Service 里面记录了每个Thread的在线用户 一个channel对应一个thread

- 群用户上线时 web server通知 channel service

channel中的某个用户上线了

- 用户断线了 push sevice也会告知 channel service

- Message Service收到用户发的信息后 找到对应的channel 把发消息的请求发给Channel service

- channel service找到当前在线的用户 然后发给push service



**如何限制多机登录**

场景：

- 不允许两个手机同时登录
- 允许手机和桌面客户端同时登录

方案：在session中记录用户的客户端信息

当用户从手机登录时，查询是否已经有其他手机处于登录状态

- 没有：创建新的session
- 有：将已有的session 删除 并发送push notification到该手机上 执行logout(发送失败也没有关系 该手机在之后访问任何API的时候 会发现自己已经logout了 并跳到登录页面)



**如何支持用户在线状态显示**

使用数据库存在线状态

onlineStatus table: user_id update_time status



用户主动向web server告知自己的状态 pull 并且顺带更新所有好友的在线状态





## 视频流系统设计

设计一个Youtube

**Scenario**

功能性需求：

- 用户上传视频
- 观看视频
- 生成缩略图

非功能性需求：

- 高可用
- 观看流畅度
- 高可靠：上传的视频不会丢失



用户上传视频所花费的存储空间和带宽计算：

1. DAU
2. 每个用户观看的视频数
3. 上传视频数与用户观看视频数的比例
4. 视频长度
5. 一分钟视频所占的存储空间 100MB
6. 每次上传视频的占用的带宽为100KB 



**Service**

- 用户服务
- **上传服务**
- 转码服务
- **缩略图服务**
- **视频服务**



**视频上传功能**：

整个文件一次上传的问题：

跟GFS文件传输一个道理 如果传输失败 需要重传整个文件

解决方案：视频切分 断点续传

- 客户端将视频切分 并对所有的chunk进行哈希 生成chunk_id

- 客户端发起上传请求 将所有chunk_id发到服务端 

  服务端返回此次上传的video_id

- 客户端开始上传chunk 

- 当客户端上传完一个chunk后 由服务端的应答告知客户端下一个应该上传的chunk_id

  断点续传也是客户端告知video_id 服务端回复chunk_id



**视频转码和缩略图生成**

转码：

- 格式转码：将用户上传的各种格式的视频 转换成Youtube兼容的格式
- 清晰度转码：一个视频 转成不同清晰度的多份视频



最终流程：

1. client -> web server : 文件上传请求

2. web server -> client： encode server地址 （避免client -> web -> encode server）

3. client -> encode server

4. encode server -> cloud storage server: chunk和缩略图

   encode server -> database：元数据 video_table chunk_table thumb_nail_table

5. web server -> database: user_video_table



**Storage**

video table：

- video_id
- hash_code：用于去重 当有重复的视频上传时 只保存第一个上传成功的
- resolution
- size
- duration
- metadata



chunk table :

- chunk_id
- video_id
- start_time
- end_time
- folder
- resolution



thumbnail table：

- thumb_id
- video_id
- moment
- folder



user table :

- user_id
- user_name



user video table:

- user_id
- video_id



存储文件的特性：

- 文件大小偏小
- 读取频率高（读写分离）



适合存储Chunks和缩略图的分布式文件系统：

- HDFS
- S3



**视频播放与缩略图加载**

如何提高观看视频的流畅度

server可以预加载client要请求的chunk及其之后的一部分chunk 类似于局部性原理



缩略图：

- 视频列表的缩略图：从文件服务上读取
- 进度条缩略图：将该视频的所有缩略图都加载到用户的本地缓存中



**Scale**

- 用CDN优化读取
- 数据库Sharding：选择sharding id



CDN：接近用户地理位置的边缘服务器，可以用作静态资源的缓存

流程：

*pull*

1. client -> CDN：请求静态资源
2. CDN -> OSS（中心服务器）: 如果没有就到中心服务器情趣
3. CDN -> client

*push*

中央服务器会定时将热门视频推到CDN上



## 设计一个Uber

> 系统设计 = 逻辑设计 + 架构设计



**Scenario**

功能性需求：

- 司机上报位置
- 用户提交打车申请 匹配到一个司机
- 司机接收或者拒绝请求
- 司机 / 用户取消请求
- 司机去接到用户 开始行程
- 到达目的地 结束行程

非功能性需求：

- QPS:

  司机DAU -> 平均在线的司机人数 -> 每隔多少秒上报一下位置 ->  报告位置的QPS（用户不用实时上报 所以可以忽略）

- 存储估算：

  - 保存所有位置信息
  - 只保存当前位置信息



**Service**

- GeoService：记录车的位置
- DispatchService：匹配打车请求 



![image-20220301075853351](/Users/yangsiping/Library/Application Support/typora-user-images/image-20220301075853351.png)



**Storage**

DispatchService：

Trip Table: 一次打车行程

- id
- user_id
- driver_id
- geohash（将二维映射到一维）
- create_time
- status



映射算法：

- Google S2

  将地址空间映射到2^64的整数  如果两个一维数比较解决 对应的二维坐标就比较接近

- Geohash

  哈希出来的字符串 公共前缀越长 两个点越接近（所以可以用数据库索引）

  公共前缀为

  6 ：误差0.6km

  5: 2.4km

  4: 20km

  原理：第一个字符表示地球地图上的一个区域 下一个字符表示这个区域内的细分区域





存储：

- SQL

  在geohash上间索引 查询的时候 like "xxx%"

- NoSQL

  可以分级存储 6 5 4前缀的 依次匹配

  Location Table：

  key(geohash prefix) -> set of drivers in this location

  



打车用户的角度：

1. 发出打车请求 查询当前位置周围的司机

   (lat,lng) -> geohash -> (driver_id1 driver_id2)

   查Location Table

2. 匹配到司机成功 查询司机所在位置

   driver_id -> (lat, lng)



司机的角度：

1. 上报自己的位置

​	(lat, lng) -> geohash(4 5 6)为位的

​	查询原来的位置 将变动的部分更新到Redis中

​	Driver Table中更新自己的最后活跃时间（心跳）

2. 接收打车请求

   修改Trip table中trip的状态

   修改driver table中自己的状态（不可用）

3. 司机完成一次trip

   trip table中修改旅程状态

   driver table中标记自己的状态进入可用状态



**Scale**

挂掉怎么办 -> 集群 + Master-slave







## MapReduce

问题：计算一个web页面中的词频

方法：

- 一台机器for loop + map

  缺点：只有一台机器 速度慢且内存大小受限

- 多态机器for loop + map

  缺点：瓶颈在于合并的机器上（即将多个map合并为一个map）

- map reduce：将数据按照key进行划分 相同的key分配到同一个机器上 这样就能进行**并行合并**

  map：将文章切分成单词 生成key-value: word-1

  reduce：将自己负责的key的value合并起来

![image-20220304122323023](https://gitee.com/yang_siping/static/raw/master/image-20220304122323023.png)



Map Reduce步骤：

- input：导入要处理的数据
- split：将要处理的数据拆分到各个执行map操作的机器上
- map：生成key-value
- partition sort：单个map机器将自己的key-value 根据key进行分区 每个分区中的数据发往不同的reduce机器 并在分区内使得数据有序（方便之后归并排序）
- fetch：reduce机器主动拉取map机器的输出（在map机器的磁盘中）
- merge sort：reduce机器将来自不同map机器的输出进行归并 key-value1 value2 value3
- reduce：将key-value1 value2 value3 合并为最后的key-value 



整体步骤图：

![image-20220304123935214](https://gitee.com/yang_siping/static/raw/master/image-20220304123935214.png)



 



