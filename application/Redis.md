# Redis

[TOC]

## 怎么学Redis 

建立系统观 不要只关注零散的技术点，抓住主线

两大维度：

- 应用
- 系统

三大主线：

- 高性能：线程模型 数据结构 持久化 网络架构
- 高可用：主从复制 哨兵机制
- 高可扩展性：数据分片 负载均衡

![image-20221114070418497](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211140704569.png)



![image-20221114070442049](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211140704068.png)





## 基本架构

> 一个key-value数据库应该包含什么



**可以存哪些数据**（数据模型）

key一般是是String Value可以是各种基本类型或复杂类型



**可以对数据做什么操作** （操作接口）

PUT GET DELETE SCAN



**数据存储**

> 数据存储在内存还是外存

缓存场景下 需要数据能支持快速访问 允许丢失 所以数据存储在内存



**访问方式**

- 通过函数库调用的方式
- 通过网络框架以Socket通信的形式对外提供键值对服务



**索引模块**

> 如何定位到键值对的位置

Redis：外层是哈希表 定位到Key-value对后 还需要从value的复杂结构（集合/列表）中进一步找到实际的元素 后者的效率依赖于数据结构



**不同操作的具体逻辑**

- GET SCAN：根据key返回value即可
- PUT: 需要为新键值分配内存空间
- DELETE：需要删除键值对 释放内存空间

使用glibc的malloc和free的问题：

由于键值对的内存大小不一 可能会造成内存碎片的问题



**重启后如何快速提供服务**

需要将数据持久化

持久化方式：

- 对于每一个Key-value 都进行写盘操作
- 将内存中的键值数据周期性地保存到文件中

![image-20221114075351540](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211140753563.png)



![image-20221114075422573](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211140754593.png)



## 数据结构

> Redis中有哪些慢操作

Redis快的原因：

- 内存操作
- 所操作的数据结构时间复杂度低



![image-20221114082010016](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211140820044.png)



**键和值用什么结构组织**

> 查找数据的第一步 找到Key-value entry的指针

哈希表，元素的key和value都是指向具体值的指针

![image-20221114083150853](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211140831880.png)



哈希表操作为什么会变慢：

- 哈希表冲突

  解决方式 拉链法 当链过长时 -> rehash

- rehash带来的操作阻塞

  默认有两个全局哈希表 rehash步骤

  1. 给哈希表2分配更大的空间
  2. 把哈希表1中的数据重新映射到并拷贝到哈希表2中
  3. 释放哈希表1的空间

  问题：步骤2 耗时过长 如果一次完成 会阻塞操作请求 造成服务一段时间内不可用

  解决方法：渐进式rehash

  处理请求时（后台也会有定时任务），将哈希表1命中的bucket的链表的所有entry都拷贝到哈希表2中

  查数据时 先查表1 查不到再查表2 

  插入数据时 只插入表2 这样表1的数据就会逐渐减少为0

  

影响集合(除String外的其他value数据类型)操作效率的因素：

- 底层的数据结构实现
- 与操作本身执行特点（get / scan）



**底层数据结构有哪些**

- 整数数组

- 双向链表

- 哈希表

- 压缩列表

  每个元素的占用空间是可变的 为了节省空间 失去了数组随机访问的特性 但是可以通过偏移量O(1)定位到头和尾 元素entry中的len也支持了双向遍历

- 跳表

  多级索引的链表



## 高性能IO模型

> 为什么单线程Redis这么快

单线程定义：Redis的网络IO与键值对读写都是一个线程完成的

其他线程负责：持久化 异步删除 集群数据同步



**使用单线程的原因**

Redis中就两张哈希表，多线程中修改共享数据结构时需要保证线程安全 进行同步操作 很多操作需要串行化 等待互斥锁 所以多线程不会更快



**单线程Redis快的原因**

- 内存操作 + 数据结构操作时间复杂度低
- 多路复用机制



**基本IO模型与阻塞点**

![image-20221114200236964](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211142002994.png)

可能的阻塞点：都可以设置为非阻塞

- accept：等待连接请求
- recv：等待数据到达



**基于多路复用的高性能IO模型**

IO多路复用：一个线程处理多个IO流

使用linux的epoll 让内核去监听多个套接字上的连接请求与数据请求 当有请求到达时 内核会生成相应的事件 放到事件队列中 Redis线程为不同事件绑定不同的处理函数 不断对事件队列中的事件进行处理即可（生产者-消费者）

![image-20221114201400199](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211142014221.png)



**单线程Redis的性能瓶颈**

1. 因为是单线程处理IO和数据读写 所以任何请求中的耗时都会影响到后面请求的响应时间
   - 操作bigkey 分配和释放内存耗时长（Redis4 lazy-free 将释放内存的操作放在了异步线程中执行）
   - 操作时间复杂度过高
   - 大量key集中过期（过期删除key是在主线程中执行的）
   - 淘汰key
   - AOF开启always 即每次都将操作写盘
   - 主从全量不同生成RDB

2. 并发量非常大，虽然采用IO多路复用，单线程从内核缓冲区拷贝数据到用户区的操作仍然是同步的（Redis6 多线程读写客户端数据 但是操作数据结构仍然是单线程的）



## AOF日志

> 宕机了 Redis如何避免数据丢失

Redis需要自身实现持久化的原因：

Redis数据存储在内存 一旦服务器宕机 数据都会丢失 从数据库中恢复会给数据库带来很大压力 并且恢复速度比较慢 影响应用的响应时间



**AOF日志实现**

属于写后日志，先写内存 然后再记录日志

![image-20221116072531710](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211160725802.png)

记录的内容：文本记录Redis收到的每一条命令

写后的原因：

- 可以保证AOF日志中的操作都是语法正确的 不用做额外的语法检查
- 避免阻塞主线程的那一次的写操作（这个很关键）

缺点：都与AOF写盘时机相关

- 写内存后 没有来得及写AOF日志 就宕机了 这份数据就会丢失
- 虽然不会阻塞当前命令 但是写日志也是在主线程中进行的 会阻塞后续的操作



**写回策略**

- always

  对于每一个写命令 都同步写日志

  可靠性最高 可以基本做到数据不丢失 性能最差 极大影响主线程的处理效率

- everysec

  写命令执行完 日志先放到AOF文件的内存缓冲区 每隔一秒刷盘

  折中

- no

  写命令执行完 只是调用write系统调用 将日志写到操作系统内核缓冲区 由操作系统控制刷盘时间

  可靠性最低 性能最好



AOF日志文件过大产生的问题：

- 文件系统对文件大小有限制
- 故障恢复时重放的时间太长



**AOF重写机制**：控制日志文件的大小

fork出一个子进程 避免主线程阻塞 创建一个新的AOF日志文件 对一个键值对只对应一个写命令 减少了日志文件的大小

重写过程：

写拷贝数据到新AOF日志文件期间的修改操作 也会写到AOF重写缓冲中 写完拷贝再应用上重写缓冲 新文件就可以替代旧文件了

![image-20221116080051050](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211160800079.png)

重写过程中的对于主线程的阻塞风险：

- fork的瞬间主线程是会阻塞的 因为需要拷贝主进程的内存页表（虚拟内存->物理内存的映射表）给子进程
- fork后主子进程共享内存空间 但是如果此时主进程中有对共享内存空间进行修改操作 则需要申请一份新的内存空间（内存分配以页为单位） 将原数据复制过去进行修改 此时也会阻塞



AOF的问题：数据恢复重放时间太长 需要执行所有命令





## 内存快照

> 宕机后 如何实现快速恢复

内存快照：将某一个时刻 内存中的数据写到磁盘中的文件里，恢复时直接读入内存即可



快照问题：

- 需要对哪些数据做快照

  内存中的所有数据 全量快照

- 做快照时 数据可以被修改吗（不能修改主线程会被阻塞）

  fork出一个子进程 子进程使用主进程的内存数据进行写快照

  ![image-20221116084513931](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211160845962.png)

- 多久做一次快照

  如果允许分钟级的丢失 可以只用RDB



RDB作为全量备份 AOF作为增量备份进行补充(redis 4)

这样快照不用执行过于频繁 AOF日志文件也不会过大 可以避免重写开销



## 数据同步

> 主从库如何实现数据一致



Redis的高可靠性：

- 数据丢失少：RDB + AOF
- 增加副本冗余 一份数据同时存放在多个实例上



如何保持多实例间数据的一致性：

主从库模式，读写分离

- 主从都可以接收读请求
- 写请求先到主库执行 然后由主库同步到从库（因为这样简单）



**主从库之间如何进行第一次同步**

![image-20221117065906887](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211170659950.png)

1. 从库与主库协商
2. 主库将RDB快照文件传输给从库 从库清空本地数据 加载RDB
3. 主库将修改操作放到replication buffer中 从库再执行这些操作



**主从级联模式分担主库的全量复制压力**

问题：每个从库与主库第一次数据同步时 主库都需要生成RDB文件(fork时会阻塞)和传输RDB文件（占用网络带宽）

解决方式：主 从 从 将压力分担给从节点

![image-20221117071401620](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211170714652.png)



写命令的传输：主从库之间维持一个长连接 避免平凡建立连接的开销

问题：网络断连怎么办

解决方法：增量复制 将断连期间的收到的命令在重连的时候 同步给从库 用偏移量记录主库写和从库读的差值 

风险：同步给从库的写命令是放在一个repl_backlog_buffer的环形缓冲区中的 如果从库读的速度显著慢与主库写的速度 命令就会被覆盖 会导致重新进行全量复制 所以需要适当调大缓冲区的空间（需要考虑写入速度与网络传输速度的差值） 



repl_backlog_buffer与replication buffer的区别

- repl_backlog_buffer主库中只有一个
- replication buffer每个client都有一个 数据先写到replication buffer再通过socket发送出去





## 哨兵机制

> 主库挂了 如何不间断服务

主库挂了的影响：

- 写操作无法进行
- 无法同步数据给从库



所以需要在主库挂了的时候 从从库中选出一个实例作为新的主库

需要解决的问题：

- 怎么判断主库是否是真的挂了（监控）
- 应该选择哪个从库作为主库（选主）
- 怎么把新主库的相关信息通知给从库和客户端（通知）



**哨兵机制**：

> 哨兵实质上也是一个Redis进程 只不过是运行在特殊模式下
>
> 主要任务是监控 选主 通知

- 监控

  定时发送PING命令到主从库 规定时间内未返回相应则判断该实例下线

- 选主

  判断出主库下线后 需要按照一定的规则从从库中选出一个实例作为主库

- 通知

  选出新主库后 将新主库的信息发给从库和客户端 重新进行连接和数据同步

![image-20221118061121587](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211180611638.png)





**主观下线和客观下线**

- 主观下线：哨兵单独判断出主库下线

  PING命令超时，如果是从库 直接标记为主观下线

  如果是主库 还需要注意是否为误判 即主库本身没有下线 只是集群网络压力大 网络堵塞 或主库压力大

  减少误判的方法：建立哨兵集群 通过多个哨兵的监控来决策主库是否下线

- 客观下线：大多数的哨兵实例都判断主库主观下线

  判断实例下限可以自行配置 一般配N/2 + 1(N为哨兵实例个数)



**如何选定新主库**

- 筛选

  网络状况：

  - 断连次数超过设定值
  - 当前不在线

- 打分

  - 从库优先级
  - 从库复制进度 slave_repl_offset最接近master_repl_offset的那个实例
  - 从库ID号



哨兵的问题：

- 哨兵集群中有实例挂了 会影响监控和选主吗（拜占庭将军问题）
- 判断出主库客观下线后 由哪个实例来执行主从切换操作（需要选举出一个leader）





## 哨兵集群

> 哨兵挂了 主从库还能切换吗



**基于pub/sub机制的哨兵集群组成**

哨兵都订阅主库的同一个消息

![image-20221118073919304](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211180739325.png)





通过INFO命令知道从库的地址

![image-20221118073848525](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211180738556.png)



问题：客户端如何知道主从切换进行的步骤



**基于pub/sub机制的客户端事件通知**

客户端可以订阅哨兵的消息

![image-20221118075656381](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211180756409.png)





**由哪个哨兵执行主从切换**

判断客观下线：投票仲裁

![image-20221118080453294](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211180804327.png)

标记客观下线后，进行Leader选举 得票半数以上同时>=quorum的哨兵成为Leader 执行主从切换操作

一个哨兵只能投一次Y，如果票数不够 说明网络情况不好 需要等待一段时间重新选举

![image-20221118082019113](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211180820145.png)





## 切片集群

> 数据增多的时候 加内存还是加实例

场景：

键值对为25G 

方案：

- 大内存云主机

  如果放在一个32G的Redis机器上 进行RDB持久化的时候 fork执行阻塞主线程的时间会很长（复制内存页表）

- 切片集群

  > 集群需要考虑的两个问题：
  >
  > - 请求路由
  > - 数据迁移（扩缩容 数据平衡）

  ![image-20221120090216474](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211200902586.png)



**保存更多数据的方式**

- 纵向扩展

  加CPU 内存 硬盘

- 横向扩展

  增加实例个数 

  扩容更容易 但是需要解决两个问题

  - 数据切片后的分布规则 set
  - 客户端怎么知道访问哪个实例拿数据 get



**数据与实例如何对应**

Redis cluster方案：

用哈希槽来维护数据与实例的映射关系

1. key -> 哈希槽

   crc16(key) % 2^16 = 哈希槽

2. 将哈希槽分配到各个实例上（可以平均 如果机器性能有差异也可以不平均）

![image-20221120092108061](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211200921100.png)





**客户端如何定位数据**

Redis实例会将自己的哈希槽->实例映射发送给其他的实例 这样每个实例都有完整的映射信息 

客户端在收到哈希槽信息后会缓存在本地



问题：哈希槽与实例的对应关系变化时 客户端如何感知

变化场景：

- 实例的增加或删除
- 因为热点key的原因 某些槽位都集中在某几个实例上 所以为了负载均衡 哈希槽需要重新分布

解决方案：重定向

重定向情况：

- 槽中的数据已全部迁移

  s1 -> c (moved 新实例IP port 同时更新客户端缓存的槽位)

  c -> s2(请求并更新本地缓存)

- 槽中的数据只是部分迁移

  s1 -> c(ask 表明查找的key不在旧实例上 在新实例上 已经迁移过去了 不会更新槽位 因为有的数据可能还在旧实例上 可以直接访问到)

  c -> s2(asking 语义是让客户端允许访问未迁移完成的槽 当成自己的槽位来处理 )

  c -> s2(get/set)





## 思考题答案与答疑

**简单的SimpleKV与Redis相比缺少什么**

![image-20221121085913805](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211210859933.png)





**整数数组和压缩列表作为底层数据结构的优势**

节省内存空间 元素之间不需要通过额外的指针连接



**Redis基本IO模型中的性能瓶颈**

任何在主线程中执行耗时过长的操作 如bigkey（其实是bigvalue） 全量返回O(n) 都是潜在的性能瓶颈



**AOF重写过程中潜在的阻塞风险**

- fork子进程的时候 需要拷贝父进程的PCB内容和页表 如果父进程的内存越大 页表就会越大 fork的时间就会越长
- copy on write：父进程和子进程共享内存空间 当父进程中要修改数据时 需要申请新的内存空间 并将旧的数据拷贝过去 如果操作的是bigkey 也会因为申请分配大内存空间而阻塞（操作系统分配内存空间 有查找和锁的开销）

TODO:去补一下操作系统分配内存空间的过程



**主从切换的时候 客户端是否可以正常进行操作**

如果主从是读写分离模式 主库故障的时候 客户端仍然可以发送读请求到从库 但是无法执行写请求



**如果想让应用程序感知不到主从切换时服务的中断 需要哨兵和客户端做什么吗**

- 客户端：

  将写请求缓存起来 给应用程序一个确认就行

- 哨兵：

  提供订阅频道 客户端订阅主从切换完成的事件

  客户端也要能主动和哨兵通信 询问主库的信息



**哨兵集群实例为5个 quorum值为2 3个实例挂了 Redis主库故障时 是否可以判断主库客观下线 是否可以进行主从库切换**

- 可以判断主库客观下线 因为还有两个实例 >= quorum
- 没有半数以上的哨兵 所以没法选出执行主从切换操作的leader



**哨兵实例是不是越多越好 调大down-after-ms值 对减少误判是不是也有好处**

- 实例越多 误判率会低 但是在判断主库客观下线和选举leader的时候 需要的票数也就越多 主库故障时 主从库切换的时间也会变长 客户端容易堆积很多的请求 可能导致请求丢失
- 调大down-after-ms 会导致判断主库故障的反应变长 影响Redis对业务的可用性



**为什么Redis不用一个表 把键值对和实例的关系记录下来**

本质上上将一个无限映射关系转化为有限映射关系（哈希的思想）

- 时间复杂度：

  当实例数量发生变动 或者数据重新分布时 修改哈希槽与实例的映射比修改所有键值对与实例映射快

- 空间复杂度：

  哈希槽个数比键值对个数少很多



**rehash的触发时机和渐进式执行机制**

- rehash时机：主要是为了避免查询链式entry
  - load factor >= 1 同时哈希表被允许进行rehash（RDB生成与AOF重写时不允许）
  - load factor >= 5

​		  load factor：entry个数 / 哈希桶个数

- 即使没有请求过来 也会定时执行一次rehash





## String内存空间消耗问题

场景：

图片ID -> 图片存储对象ID（两个都是10位数）

一亿个键值对 占了6.4GB内存，会导致大内存Redis实例因为生成RDB而导致响应慢的问题

问题：占有内存空间过多



解决方式：通过二级编码 用节省内存空间的集合类型来保存单值键值对



**String类型内存开销大的原因**

需要记录额外的元数据:

- 数据长度
- 空间使用



RedisObject：统一不同数据

- 元数据
- 实际数据的指针

![image-20221121221747365](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211212217442.png)



String 不同类型时的内存存储：（key和value都是）

- Long类型 RedisObject不存储指针 而是直接存储值 避免额外的空间开销与时间开销
- 字符串数据 且字符串<=44字节 元数据 指针 SDS是一块连续的内存 为的是避免内存碎片
- 字符串数据 且字节>44B SDS单独分配空间

![image-20221121222300859](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211212223892.png)



所以图片ID与图片存储对象ID一个键值对应该是：

16（元数据 + key的int编码） + 16（元数据 + value的int编码） = 32B

但是实际上是64B

因为全局哈希表的dictentry需要额外的3个8B的指针，加上redis用jemalloc分配内存 会以最靠近要求单位的2的幂方来分配



![image-20221121223507896](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211212235937.png)

所以16B的数据需要64B的空间进行存储



**应该用什么数据结构可以节省内存**

压缩列表：

![image-20221121224503234](/Users/yangsiping/Library/Application Support/typora-user-images/image-20221121224503234.png)

用一系列连续的entry来保存数据，不用额外的指针进行连接

entry组成：

- prev_len：前一个entry的长度

  1B或5B

- len：自身长度

  4B

- encoding：编码方式

  1B

- content：实际数据



**如何用集合类型保存单值的键值对**

采用基于hash类型的二级编码，将key分成两个部分 前一个部分做完hash类型的key后一部分作为hash集合的value

```shell
127.0.0.1:6379> info memory
# Memory
used_memory:1039120
127.0.0.1:6379> hset 1101000 060 3302000080
(integer) 1
127.0.0.1:6379> info memory
# Memory
used_memory:1039136
```



Hash类型在没有超过阈值的时候是使用压缩列表来存储来存储 超过了是用哈希表

阈值:

- 元素个数
- 单个元素的最大长度





## 需要统计一亿个keys时，应该用哪种集合

场景：

- 移动应用中每天的用户登录信息：

  日期 -> 用户ID/设备ID

  需要统计每天新增用户与第二天的用户留存

- 电商网站商品的用户评论列表：

  商品 -> 一系列的评论

  需要统计评论列表中的最新评论

- 签到打卡：

  日期 -> 一系列用户的签到信息

  需要统计一个月内连续打卡的用户数

- 应用网站上的网页访问信息：

  网页 -> 一系列的访问点击

  需要统计独立访客量



集合类型的统计模式：

- 聚合
- 排序
- 二值状态
- 基数



**聚合统计**

求交集 差集 并集

每天新增用户统计步骤：

1. 用一个集合set1记录所有登录过app的用户ID
2. 另一个集合set2记录每天登录的用户ID
3. 新增用户 = set2 - set1

用户第二天留存：

日期set day2 ∩ 日期set day1



风险：

set的差集 交集 并集计算复杂度比较高 数据量大的情况下 直接执行计算会阻塞Redis 计算最好放在从库或者是客户端



**排序统计**

求评论列表中的最新评论

所以要求元素是按时间排序的

可以选的集合类型：

- List:

  按照元素进入List的顺序进行排序的

  问题：

  分页查询遇到动态数据时 会出现问题

  ```shell
  # A B C D E F
  LRANGE product1 0 2
  1) "A"
  2) "B"
  3) "C"
  
  
  LRANGE product1 3 5
  1) "D"
  2) "E"
  3) "F"
  
  # 当插入了一个新数据G 第二页评论C又被展示出来了
  LRANGE product1 3 5
  1) "C"
  2) "D"
  3) "E"
  ```

  

- Sorted Set:

  可以根据元素的权重来排序 如根据时间来决定权重值

  优点：

  分页查询时 不会出现重复的元素

> 例子：
>
> 假设当前的评论 List 是{A, B, C, D, E, F}（其中，A 是最新的评论，以此类推，F 是最早的评论，权重分别为 10，9，8，7，6，5）。 在展示第一页的 3 个评论时，按照权重排序，查出 ABC。 展示第二页的 3 个评论时，按照权重排序，查出 DEF。 如果在展示第二页前，又产生了一个新评论 G，权重为 11，排序为 {G, A, B, C, D, E, F}。 再次查询第二页数据时，权重还是会以 10 为准，逻辑上，第一页的权重还是 10，9，8。 查询第二页数据时，可以查询出权重等于 7，6，5 的数据，返回评论 DEF。 当想查询出最新评论时，需要以权重 11 为准，第一页数据的权重就是 11，10，9，返回评论 GAB。 再次查询第二页数据时，以权重 11 为准，查询出评论 CDE。



**二值状态统计**

bitmap：一个bit 1 0表示二级状态 海量数据时  可以节省内存空间

```shell
# uid为3000的用户 8月3号签到
SETBIT uid:sign:3000:202008 2 1 

# 查询该用户8月3号是否签到
GETBIT uid:sign:3000:202008 2 

# 统计该一个月内的签到次数
BITCOUNT uid:sign:3000:202008
```



统计一亿个用户连续10天都签到的用户数步骤：

1. 10天用10个bitmap 每个bitmap一亿位 每一位表示一个用户当天的签到情况
2. 对10个bitmap 做相与操作 统计出bitmap中1的个数



**基数统计**

统计一个集合中不重复的元素个数

网页UV统计：一个用户一天内多次访问只能算作一次

可用的集合：

- set 去重 但是内存空间消耗大
- hash  user -> 1 内存空间消耗同样大
- hyperLogLog 12K内存就可以统计2^64个元素的基数 统计基于概率 会有一定的误差

![image-20221122062545654](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211220625693.png)





## GEO是什么 

场景：

搜索附近的餐馆 附近的出租车

LBS应用 location-based service

访问的数据是人或物关联的一组经纬度信息



**GEO的底层结构**

设计一个数据结构时 需要考虑数据的访问特点

访问特点：

- 车把自己的网约车编号和经纬度信息给叫车应用
- 用户在叫车时 叫车应用会根据用户的经纬度信息查找用户附近的车辆 并进行匹配
- 匹配上后 叫车应用就会根据车辆的编号 获取车辆的信息 并返回给用户

选择数据类型：

- Hash

  可以快速更新车辆变化的经纬度信息

  但是不支持范围查询 因为元素不是有序的

- Sorted Set:

  key是元素  value是权重分数

  可以根据元素的权重分数排序 支持范围查询 这就能满足LBS服务中查找相邻位置的请求

  ![image-20221123075913526](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211230759606.png)



问题：

经纬度是二维信息，而Sorted Set元素的权重分数是一个浮点数 怎么把二维信息转成一维的 且空间上相近的点 在一维数值上也是相近的



解决方法：

**GeoHash的编码方法**

对经度和纬度分别编码 然后把经纬度各自的编码组合成一个最终编码

做法：二分区间 区间编码

[116.37,  39.86]

![image-20221123080955193](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211230809234.png)

![image-20221123081025093](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211230810127.png)

[11010,  10111]

组合在一起：偶经 奇纬

![image-20221123081305704](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211230813748.png)



相当于把地理空间划分成了一个个方格

![image-20221123081541375](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211230815414.png)



**操作方式**

```shell
# 将一组经纬度信息和一个ID 记录到GEO类型集合中
GEOADD cars:locations 116.034579 39.030452 33

# 查找以这个经纬度为中心的5km内的车辆信息 按从近到远的方式排序
GEORADIUS cars:locations 116.054579 39.030452 5 km ASC COUNT 10
```





## 如何在Redis中保存时间序列数据

场景：

物联网中需要周期性同步设备的实时状态

```shell
DeviceID, Pressure, Temperature, Humidity, TimeStamp
```



**时间序列数据的读写特点**

- 写：

  持续高并发写入 都是插入新数据 插入之后不会变更

  所以要求数据插入时 复杂度要低 尽量不要阻塞

- 读：

  查询某个设备 某一个时刻的运行状态

  查询某个时间范围内的状态

  对某个时间范围的数据做聚合计算：均值 最大 最小 求和



**考虑的数据类型**

- Hash + Sorted Set

  Hash满足查询某个设备某个时刻的数据 但是不支持对数据进行范围查询

  ![image-20221123090539226](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211230905320.png)

  Sorted Set可以满足按时间范围查询的需求

  问题：如何保证写入Hash和Sorted Set是一个原子操作

  可以用multi和exec来保证

  ![image-20221123091605026](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211230916070.png)

  问题：如何对时间序列数据进行聚合计算

  只能把时间范围内的数据取回到客户端 然后在客户端完成聚合计算 风险是大量数据在Redis实例与客户端之间传输 会和其他操作竞争网络资源

- RedisTimeSeries

  扩展模块 支持一个事件范围的内的聚合计算





## Redis对消息队列的支持

问题：Redis适合做消息队列吗

- 消息队列的存取需求是什么
- Redis如何实现消息队列的需求



**消息队列的消息存取需求**

- 消息保序（需要消息数据有序存取）

  消费者需要按生成者发送消息的顺序来处理消息

- 处理重复的消息（需要消息数据具有全局唯一编号）

- 保证消息可靠性（需要消息数据在消费完成之后才从消息队列中被删除）

  消费者重启后 可以重新读取未处理完的消息再次进行处理



**基于List的消息队列解决方案**

- 消息保序：

  lpush + rpop

  问题：

  rpop是非阻塞的 在没有数据时会返回空值

  所以需要在程序while循环中一直调用

  解决方式：

  使用阻塞获取方法brpop 可以节省CPU开销

- 处理重复的消息

  消费者需要用判断出重复消息的能力：

  - 消息队列要能给每一个消息提供全局唯一的ID号
  - 消费者程序要把已经处理过的消息的ID号记录下来

  用List的时候 只能生产者自己手动在消息内加上唯一ID

- 消息可靠性

  brpoplpush 消费者在取出消息的同时 消息会放入redis的另一个list中 消费者宕机重启的时候可以从备份list中再次读取消息进行处理

  ![image-20221125095616775](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211250956932.png)

问题：

当生产速率 > 消费速率时  List中堆积的消息就会越多 会给Redis内存带来压力

所以希望有多个消费者组成一个消费者组（单单的多个消费者没有办法处理重复消息的问题）





**基于Streams的消息队列解决方案**

提供的命令：

- XADD：插入消息 保证有序 可以自动生成全局唯一ID(消息有序 不重复消费)

- XREAD：用于读取消息 可以按ID读取数据

- XREADGROUP：可以按消费组形式读取消息

- XPENDING XACK：（消息可靠性）

  XPENDING：查询消费组内所有消费组已读取但未确认的消息

  XACK：向消息队列确认消息处理完成

![image-20221125101500469](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211251015581.png)

> 一个消息需要同时被多个消费者进行处理 那就需要用Streams类型 建立多个消费者组





## 异步机制：如何避免单线程模型的阻塞

**Redis实例与不同对象交互时会发生的动作**

- 客户端：

  网络IO 键值对增删查改 数据库操作

- 磁盘：

  生成RDB快照 记录AOF日志 AOF日志重写

- 主从节点：

  主库生成 传输RDB文件 从库接收RDB文件 清空数据库 加载RDB文件

- 切片集群实例

  向其他实例传输哈希槽信息 数据迁移



**Redis实例有哪些阻塞点**

- 集合全量查询以及聚合操作

  可以用scan命令 分批读取数据

- bigkey删除 释放内存的操作：操作系统需要将释放掉的内存块插入到空闲内存块的链表中

  可以scan读取命令 再分批进行删除

- 清空数据库（flushdb flushall）

- AOF日志同步写

- 从库加载RDB文件

  主库的数据量大小控制在2-4G



解决方案：不在关键路径上的操作 都可以放到异步线程中执行

> 关键路径上的操作：客户端把请求发送给Redis后 等着Redis**返回数据结果**的操作 如读操作

所以bigkey删除(unlink) 清空数据库(flushdb async) AOF日志同步写都可以异步执行

> 



**异步的子线程机制**

放入任务队列中即返回

![image-20221126155019529](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211261550618.png)





## 为什么CPU结构也会影响Redis性能

**主流的CPU架构**

- CPU多核

  ![image-20221126160221929](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211261602983.png)

- 多CPU Socket（NUMA架构 应用程序访问本地内存与访问远端内存的延迟不一致）

  ![image-20221126160502787](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211261605836.png)

CPU架构对应用程序的影响：

- 访问L1 L2中缓存的指令和数据速度很快 充分利用L1 L2缓存 可以有效缩短应用程序的执行时间
- NUMA架构下 应用程序从一个Socket调度到另一个Socket上 会出现远端内存访问的情况 会直接增加应用程序的执行时间



**CPU多核对Redis性能的影响**

应用程序在一个CPU核执行时 会将运行时信息存储在CPU核的寄存器上，同时访问最频繁的指令和数据会被缓存到L1 L2缓存上，当应用程序被调度到其他CPU核时(CPU context switch) 运行时信息 L1 L2缓存都需要重新加载

解决方法：绑核 

```shell
# 将Redis实例绑到了0号核上
taskset -c 0 ./redis-server
```

![image-20221126170623460](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211261706514.png)



**CPU的NUMA架构对Redis性能的影响**

数据从网络中读取到Redis内存的流程：

1. 网络中断处理程序从网卡硬件中读取数据

   并把数据写入到内核的内存缓冲区

2. 内核通过epoll机制触发事件 通知Redis实例 

3. Redis实例把数据从内核的内存缓冲区拷贝到自己的内存空间

![image-20221126171039218](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211261710264.png)



问题：当网络中断处理程序和Redis实例各自绑定的CPU核不在同一个CPU Socket时 Redis读取网络数据时 就需要跨CPU Socket访问内存

![image-20221126174424503](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211261744555.png)

解决方法：将网络中断处理程序和Redis实例帮在同一个CPU Socket的不同核上



绑核的风险：当把Redis实例绑在一个CPU逻辑核上时 会导致主线程 后台线程 子进程竞争CPU资源

解决方法：

- 把Redis实例绑在一个物理核上（即绑对应的两个逻辑核）

- 优化Redis源码 生成线程/子进程时 将其绑到不同的CPU核上

  ```c
  
  //线程函数
  void worker(int bind_cpu){
      cpu_set_t cpuset;  //创建位图变量
      CPU_ZERO(&cpu_set); //位图变量所有位设置0
      CPU_SET(bind_cpu, &cpuset); //根据输入的bind_cpu编号，把位图对应为设置为1
      sched_setaffinity(0, sizeof(cpuset), &cpuset); //把程序绑定在cpu_set_t结构位图中为1的逻辑核
  
      //实际线程函数工作
  }
  
  int main(){
      pthread_t pthread1
      //把创建的pthread1绑在编号为3的逻辑核上
      pthread_create(&pthread1, NULL, (void *)worker, 3);
  }
  ```

  



## 波动的响应延迟：如何应对变慢的Redis (上)



**如何判断Redis是否真的变慢了**

- 查看Redis的响应延迟 如执行命令突然增长到了几秒

- 基于当前环境的Redis基线性能进行判断 当观测到的Redis运行时延迟是基线性能的2倍以上 就可以认定Redis变慢了

  ```shell
  # 在服务端直接执行 不考虑网络情况 120s内观测到的最大延迟为119微秒 0.119毫秒
  ./redis-cli --intrinsic-latency 120
  Max latency so far: 17 microseconds.
  Max latency so far: 44 microseconds.
  Max latency so far: 94 microseconds.
  Max latency so far: 110 microseconds.
  Max latency so far: 119 microseconds.
  
  36481658 total runs (avg latency: 3.2893 microseconds / 3289.32 nanoseconds per run).
  Worst run took 36x longer than the average latency.
  ```

  

**如何应对Redis变慢**

排查要素：

- Redis自身操作特性
- 操作系统
- 文件系统



**Redis自身操作特性的影响**

- 慢查询

  通过日志定位是否有复杂度高的慢请求 有则用其他高效命令代替或者复杂操作（排序 交集 并集）在客户端进行

- 过期Key

  同一时刻内有大量key过期 需要被删除

  所以在业务逻辑中 如果要在同一个时刻内生成大量过期时间相同的key 可以加上一定大小范围的随机数（还是分摊的思想）





## 波动的响应延迟：如何应对变慢的Redis（下）



**文件系统：AOF模式**

涉及到的两个系统调用：

- write：

  将日志记录写到内核缓冲区就返回了 不需要等待日志实际写到磁盘

- fsync：

  需要等待日志记录写到磁盘后才返回 时间较长

![image-20221128162216013](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211281622136.png)

- everysec：

  fsync在子线程中异步执行，但是如何主线程发现上一次的fsync还没有执行完时 也会阻塞

  AOF重写时 磁盘IO压力大 会阻塞住everysec异步子线程的fsync 从而阻塞住主线程

  解决方法：

  - 如果业务对数据可靠性要求不高可将写回策略设置为no

  - 如果业务对延迟很敏感 同时允许一定量的数据丢失

    可以设置no-appendfsync-on-rewrite yes 让AOF重写的时候 不进行fsync操作

  - 采用固态硬盘

- always：fsync在主线程中执行

![image-20221128162558081](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202211281625148.png)





**操作系统：**

**swap**

原因：物理机器内存不足

- redis实例自身使用了大量内存
- 与redis在同一机器的其他进程在进行大量文件读写 占用了系统内存

解决方式：

- 增加机器内存（纵向扩展）
- 使用redis集群（横向扩展）
- 迁移Redis实例到单独的机器上





**内存大页**

好处：分配相同内存量时 可以减少分配次数

对Redis的坏处：生成RDB文件的时候 用的fork 此时主进程修改内存的数据时 申请和拷贝内存的单位会从4KB -> 2MB的页 一般需要关掉





## 删除数据后 为什么内存占用率还是很高

场景：数据删除后 redis释放的内存空间会给内存分配器管理 不会立即返回给操作系统 

风险：产生内存碎片

**内存碎片的形成**

- 内因：jemalloc内存分配器的内存分配机制

  按照固定大小分配 8B 16B 32B... 好处是可以减少分配次数 比如申请20B 实际会给32B 应用再写10B的数据时 就不用再向操作系统申请分配空间了

- 外因：键值对大小不一样和修改删改操作



**如何判断是否有内存碎片**

```shell
INFO memory
# rss为操作系统实际分配的物理空间 其中包含了碎片
# used_memory 为保存数据实际使用的空间
# 1-1.5为正常值
mem_fragmentation_ratio = used_memory_rss/ used_memory
```



**如何清理内存碎片**

- 重启redis
- 自动清理：在主线程中执行 需要通过参数控制碎片清理的开始和结束实际 占用的CPU比例 减少碎片清理对Redis本身请求处理的影响



## 缓冲区

作用：

- 协调生产者与消费者之间的速率
- 累积进行批处理

风险：生产速率 > 消费速率时 缓冲区溢出



Redis场景：

- 客户端与服务端
- 主节点与从节点



**客户端输入和输出缓冲区**

服务端：为**每个**客户端都分配了输入和输出缓冲区

- 输入缓冲区：缓冲命令
- 输出缓冲区：缓冲结果和数据

客户端：

缓冲命令，一是批处理 一次发送多个命令（Pipeline） 减少网络消耗 二是可以在主节点故障进行主从切换无法响应的时候 缓存应用的操作 避免让应用感知到Redis服务不可用

![image-20221204135329416](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202212041353559.png)



**如何应对输入缓冲区溢出**

观察方法：

```shell
# cmd 客户端最新执行的命令
# qbuf 输入缓冲区已使用的大小
# qbuf-free 输入缓冲区剩余大小
127.0.0.1:6379> CLIENT LIST
id=5 addr=127.0.0.1:59968 laddr=127.0.0.1:6379 fd=8 name= age=2 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 qbuf=26 qbuf-free=16864 argv-mem=10 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=34042 events=r cmd=client|list user=default redir=-1 resp=2
```

溢出后果：Redis关闭客户端连接

解决方法：目前没有方法调大输入缓冲区

- 生产者：

  避免写入bigkey

- 消费者：

  避免Redis主线程阻塞



**如何应对输出缓冲区溢出**

组成：

- 16KB的固定空间：用于暂存OK响应和出错细腻系
- 动态增加的缓冲空间：暂存响应结果

解决方法：

- 避免bigkey操作返回大量数据结果
- 避免在线上环境持续使用MONITOR命令
- 设置client-output-buffer-limit 设置合理的缓冲区大小上限



**主从集群中的缓冲区**

- 复制缓冲区（全量复制时使用）

  为每一个从节点都维护一个 用于暂存传输RDB过程中 主节点接收到的写命令请求

  ![image-20221204141845795](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202212041418844.png)

  溢出场景：从节点接收和加载RDB较慢 主节点接收到了大量的写命令

  溢出风险：关闭和从节点之间进行复制操作的连接

  解决方法：

  - 控制主节点保存数据量的大小 2-4G
  - 调整复制缓冲区大小

- 复制积压缓冲区（增量复制主从节点网络断连 恢复连接后使用）

  主节点中只有一个

  主节点在将写命令同步给从节点的时候 也会将写命令写入复制积压缓冲区

  ![image-20221204145601536](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202212041456585.png)



## 思考题与答疑

**一个生产者的消息需要被多个消费者消费的场景 Redis怎么做**

- Streams 建立多个消费组
- pubsub



**Redis的写操作是在关键路径上吗**

在，不管客户端关不关心写操作的成功 只要上一个命令的结果没有返回 就不能发送下一个命令



**一台有两个CPU Socket（每个Socket 8个物理核）的服务器上 部署了8个Redis实例的切片集群**

绑核方案：两个CPU Socket各自运行4个实例

原因：

- CPU缓存：

  都放在一个CPU Socket中 会竞争L3缓存 导致命中率降低

- 内存：

  都放在一个CPU Socket 也会竞争内存资源 内存不够时 会跨Socket申请内存 导致跨Socket访问内存 降低性能



**如何排查慢命令**

- 慢查询日志

  slowlog-log-slower-than

  slowlog-max-len

- latency monitor



**如何排查Redis中的bigkey**

- redis-cli --bigkeys：输出每种类型最大的bigkey信息以及平均大小
- 自己开发程序scan扫描统计



## 旁路缓存

**缓存的特征**

- 可以避免从慢速子系统存取数据的一个快速子系统

  如CPU cache之于内存

  内存 page cache之于磁盘

- 缓存系统的容量总是小于慢速系统的（所以需要淘汰策略）

  

​		

**Redis缓存处理读请求的两种情况**

- 缓存命中：直接读取

- 缓存缺失：

  需要从数据库中读取 同时需要把缺失的数据写入Redis（缓存更新 涉及到缓存和数据库之间的数据一致性问题）

  

**Redis作为旁路缓存的使用操作**

> 旁路缓存(cache aside)：读缓存 读数据库 更新缓存的操作都需要在应用程序中进行
>
> 像CPU缓存和page cache是read through cache 
>
> ## Cache Aside Model
>
> Request data from the cache using a key. If cache has the data, return it to the caller immediately. If there is no data, we request the persistent storage (DB) for data. After getting the data from the DB, we put it back in the cache with the key. And return the data. We call this `Cache Aside` model. It is the default way of working with cache for most applications.
>
> ## Read Through Cache
>
> In `Read Through Cache`, the application always requests data from the cache. If cache has no data, it is responsible for retrieving the data from the DB using an underlying provider plugin. After retrieving the data, the cache updates itself and returns the data to the calling application.



**缓存类型**

- 只读缓存：只加速读请求 如短视频app中的视频属性 确定之后 修改不频繁

  对于读请求 会先调用redis get

  对于写请求 会直接发往数据库 删改的数据如果在redis中存在 则需要把数据从缓存中删除 下次读发现没有 则从数据库中加载 然后添加到redis中（先删改数据库 然后再删redis 保证数据一致性）

- 读写缓存：读写请求都加速 如商品大促时 需要修改商品的库存信息

  对于读请求 先调用redis get

  对于写请求 先在缓存中进行增删改（会有数据丢失的风险）

  写回策略：写数据库的时机不同

  - 同步直写：侧重于数据可靠性

    写缓存时 同时也写数据库 等都成功写完之后 才给客户端返回

  - 异步写回：侧重于快速响应

    增改的数据要被从缓存中淘汰出来的时候 再写数据库



## 替换策略

> 缓存满了怎么办

为什么不把要访问的数据全部放在缓存中 加快访问速度：

性价比问题 由于数据访问的局部性 80%的请求实际只访问了20%的数据 把全部的数据都放缓存中 性价比太低 所以缓存的容量必然小于数据库的数据量 所以当缓存写满时 就需要淘汰策略 



**缓存容量设置多大合适**

需要结合数据实际访问特征和成本 一般设置为数据总量的15-30%



**Redis中的淘汰策略**

淘汰策略需要处理的问题：

- 如何选择出需要淘汰的数据
- 如何处理被淘汰的数据

策略分类：

![image-20221206085256115](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202212060852226.png)



**noeviction**

当Redis内存空间超过maxmemory时 不淘汰数据 之后再有写请求过来 返回错误



**在设置了过期时间的数据中进行淘汰**

删除时机：

- maxmemory从设置了过期时间中的key找
- key到达过期时间



分类：

- volatile-random：随机删除
- volatile-ttl：按照过期时间的先后进行删除 先删最早过期的
- volatile-lru：使用LRU算法筛选
- volatile-lfu：使用LFU算法筛选



**在所有数据中进行淘汰**

删除时机：

- maxmemory
- key到达过期时间



分类：

- allkeys-random：从所有键值对中随机选择并删除
- allkeys-lru：使用LRU算法筛选
- allkey-lfu：使用LFU算法筛选



**LRU**

*Least Recently Used*

![image-20221206091745299](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202212060917339.png)

传统实现的弊端：

- 空间：需要额外维护一个全局数据的LRU链表
- 时间：每次数据访问都需要操作链表 将数据移动到MRU端



Redis中的简化：维护一个小链表

1. 由RedisObject中的lru字段记录每个数据最近被访问的时间戳
2. redis维护一个maxmemory-samples大小的候选集 第一次先随机选出maxmemory-samples个键值对 当内存超出时 淘汰lru时间戳小的key
3. 后续放入候选集的key都必须小于 候选集中的最小lru  如果候选集慢了 就淘汰掉lru较大的key



**淘汰策略使用建议**

- 业务数据都有冷热之分：优先使用allkeys-lru策略
- 业务数据访问频率相差不大：使用allkeys-random
- 业务中有置顶需求：volatile-lru 同时不给置顶数据设置过期时间（这样数据就一直不会被删除）



**如何处理被淘汰的数据**

一般的缓存系统 对于脏数据在被淘汰时 是需要写回数据库的 

但是Redis是直接删除了 没有写回数据库的操作 所以需要应用程序在数据修改的时候就需要写到数据库



## 缓存异常（上）：如何解决缓存和数据库的数据不一致问题

**缓存和数据库的数据不一致是如何发生的**



> 数据一致性的定义：
>
> 1. 当缓存中有数据时 缓存的数据要和数据库的数据一致
> 1. 当缓存中没有数据时 数据库中的值必须是最新值





缓存读写模式：

- 读写缓存

  - 同步直写

    写缓存后 也同步写数据库（需要用分布式事务来保证原子性）

  - 异步写回

    写缓存后 等待数据要被淘汰时 再写数据库（有数据不一致的风险）

- 只读缓存

  只读缓存的好处是能够保证缓存中的数据都是用户访问加载到缓存里的数据 相对而言比较热点 如果修改数据库的同时也修改缓存 虽然可以减少加载环节 但是可能会导致缓存命中率不高

  - 新增数据：

    数据直接写到数据库中 缓存中无数据 数据库中有数据 符合第一种情况

  - 删改数据：

    更新数据库 同时也要删除缓存 这两个操作需要保证原子性 这两个操作无论谁先做 如果第一个成功 第二个失败都会读到旧值



只读缓存不一致情况：

- 修改数据库和删除缓存这两个操作不是原子性 第一个成功 第二个失败
- 在两个步骤之间 有请求进来访问数据



**如何解决只读缓存数据不一致问题**

- 重试机制

  第一步操作成功后 将第二步要进行的操作放到消息队列中 如果失败则从队列中取出消息进行重试 重试失败达到一定次数向业务层报错 第二步成功后 从队列中删除消息

  ![image-20221206192544987](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202212061925087.png)

- 延迟双删：当先删除缓存 再修改数据库时

  > 一般是先db 后缓存 删除缓存的速度很快 并发小的情况 不会有太多请求读到缓存中的旧值 对业务的影响很小
  >
  > 如果一定要保证数据一致性 可以暂存读请求 用分布式锁

  当有其他线程在删除缓存后访问应用 则会造成旧值被重新加载到缓存中 此时数据不一致

  ```java
  // 删除线程
  redis.delKey(X)
  db.update(X)
  // sleep的时间 需要保证其他线程将旧值从db中读出来并写到缓存中
  Thread.sleep(N)
  redis.delKey(X)
  ```



## 缓存异常（下）：如何解决缓存雪崩、击穿、穿透

**缓存雪崩**

> 大量的请求无法在缓存中处理 应用会将大量的请求发送到数据库层 导致数据库压力激增
>
> redis的并发处理能力是万级 mysql的并发处理能力是千级

导致原因：

- 缓存中有大量数据同时过期

  应对方法：

  - 避免给大量的数据设置相同的过期时间 如果必须要 则可以给固定的过期时间增加一个随机数
  - 服务降级：只有核心数据过期了才会去请求数据库
    - 对于非核心数据：如商品属性 缓存都不查 直接返回预定义信息
    - 对于核心数据：流程照常 先缓存 再数据库

- Redis实例宕机

  应对方法：熔断和限流都是已经雪崩的事后措施

  - 服务熔断

    缓存客户端不把请求发给redis实例 而是直接返回 等到实例恢复后 再允许应用系统发送请求到缓存系统

  - 请求限流

    在请求入口前端将流量控制到单靠数据库就能承受的数量

  - 搭建Redis主从集群+哨兵（预防式方案 尽量使用预防式方案）



**缓存击穿**

> 热点数据过期失效

解决方法：不给热点数据设置过期时间



**缓存穿透**

> 指要访问的数据既不在缓存中 也不在数据库中

原因：

- 业务层误操作 将数据从缓存和数据库中都删除了
- 外部恶意攻击 专门访问不存在的数据

方法：

- 针对查询的key 在redis中缓存一个空值或默认值
- 使用布隆过滤器判断数据是否在数据库中存在 减轻数据库压力
- 在请求入口的前端 对业务系统接收到的请求进行合法性检测



## 缓存被污染了怎么办

**缓存污染**：

缓存中存在着大量访问次数非常少 甚至只会访问一次的数据 这样会造成缓存命中率低且占用缓存空间 且如果占满了缓存的话 当要往缓存写入新数据时 需要先淘汰掉这些数据 带来额外的时间开销

一般由扫描式单次查询导致



**如何解决缓存污染问题**

淘汰数据时 优先淘汰不会再被访问的数据



**淘汰策略的选择**：

- volatile-ttl

  当提前知道数据的被访问时长时 可以使用 适合短时高频的数据

- lru：更加关注数据的时效性

  无法解决 因为是根据访问时间 缺少了访问次数的维度

- lfu：更加关注数据的访问频次

  可以解决 增加了访问次数的维度 先选出访问次数最小的 当访问次数相同时 选择访问时间最久的

  实现：

  在lru的基础上 将lru 24bit时间戳字段拆成两个部分 

  16bit：

  表示访问时间（分钟级）

  8bit：

  访问次数 0-255 非线性增长 类似于取log lfu_log_factor参数控制

  也会随着当前时间与最近访问时间的差值增加而下降 lfu_decay_time控制



## Pika：如何基于SSD实现大容量Redis

场景：Redis需要保存更多数据

解决方法：

- 横向扩容：建立Redis切片集群 

  缺点：运维复杂 

- 纵向扩容：增加Redis单机内存

  缺点：会生成较大的RDB文件  从而导致 生成RDB时间长 fork时阻塞主线程 通过RDB进行恢复的时间长 主从节点同步时同步RDB文件给子节点的时间长 复制缓冲区易溢出  主从切换开销大（切换完主节点后 所有从节点需要重新消费新的主节点的RDB文件）

另外的方案：使用SSD作为存储 Pika

Pika的设计目标：

- 单实例可以保存大容量数据 同时避免实例恢复和主从同步的问题
- 兼容Redis数据类型





**Pika如何基于SSD保存更多数据**

RocksDB + binlog

- RocksDB的写入流程

  ![image-20221209193745181](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202212091937302.png)

  读流程

  ![image-20221209194016818](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202212091940878.png)



## 无锁的原子操作

> Redis如何应对并发访问

场景：多个用户同时下单某个商品 对Redis中的商品库存进行并发更新

这种情况下保证并发安全的方式：

- 加锁

  弊端：

  - 串行化操作 降低系统并发访问性能
  - 需要分布式锁 实现复杂

- 原子操作

  执行过程保持原子性的操作



**Redis的两种原子操作方法**

- 单命令操作 多个操作在Redis中实现成一个操作

  因为Redis本身是用单线程来处理客户端请求操作的 所以命令操作是互斥的 

  可以用incr/decr 来操作库存

  缺点：不能执行更加复杂的判断逻辑和其他操作

- Lua脚本：多个操作写到一个lua脚本中 用EVAL命令执行脚本可以保证原子性

  

场景：一分钟内最多允许某个用户访问60次

```lua
//获取ip对应的访问次数
current = GET(ip)
//如果超过访问次数超过20次，则报错
IF current != NULL AND current > 20 THEN
    ERROR "exceed 20 accesses per second"
ELSE
    //如果访问次数不足20次，增加一次访问计数
    value = INCR(ip)
    //如果是第一次访问，将键值对的过期时间设置为60s后
    IF value == 1 THEN
        EXPIRE(ip,60)
    END
    //执行其他操作
    DO THINGS
END
```



```lua
# 可以保证3个操作的原子性 避免把不需要做并发控制的操作写入脚本 跟细化锁的粒度一个道理
local current
current = redis.call("incr",KEYS[1])
if tonumber(current) == 1 then
    redis.call("expire",KEYS[1],60)
end

# 执行lua脚本 建议先使用script load预先将lua脚本加载到redis中 得到一个脚本摘要值 再通过evalsha命令+脚本摘要值来执行脚本 可以避免脚本传输的网络开销
redis-cli  --eval lua.script  keys , args
```



## 如何使用Redis实现分布式锁

**单机环境的锁与分布式环境的锁的联系与区别**

单机下是操作单机内存中的一个变量值

```c
// 一般通过CAS来保证原子性
acquire_lock(){
  if lock == 0
     lock = 1
     return 1
  else
     return 0
} 

release_lock(){
  lock = 0
  return 1
}
```

分布式环境下：

锁变量需要一个可以被多个客户端访问的共享存储系统维护

要求：

- 原子性：

  因为分布式加锁和解锁都涉及多个操作 所以实现分布式锁时需要保证这些操作的原子性

- 可靠性：

  需要保证共享存储系统的可靠性



**基于单个Redis节点实现分布式锁**

保证原子性的方式：

- 单命令操作 setnx（读 判断 写）

- lua脚本



```lua
// 单命令SETNX加锁
SETNX lock_key 1
// 业务逻辑
DO THINGS
// 释放锁
DEL lock_key
```

存在的问题：

- 某个客户端加锁成功后发生异常 没有释放锁 此时其他的客户端都无法获取锁
- 其他客户端也可以删除另外的客户端加的锁

解决方法：

- 给锁变量设置一个超时时间 超时时间应该设置为大于执行业务逻辑的时间 这样就算持有锁的客户端发生异常 超时时间到了之后 其他客户端还是可以正常获取锁

  ```lua
  SET key value [EX seconds | PX milliseconds]  [NX]
  
  // 加锁, unique_value作为客户端唯一性的标识
  SET lock_key unique_value NX PX 10000
  ```



- 将锁的value设置为加锁的客户端的唯一标识 释放锁时 先判断当前客户端是否持有锁 涉及多个操作 所以要保证原子性需要用到lua脚本

  ```lua
  //释放锁 比较unique_value是否相等，避免误释放
  if redis.call("get",KEYS[1]) == ARGV[1] then
      return redis.call("del",KEYS[1])
  else
      return 0
  end
  redis-cli  --eval  unlock.script lock_key , unique_value 
  ```

问题：该Redis宕机后 锁变量就没有了 客户端就无法进行锁相关操作了



**基于多个Redis节点实现高可靠的分布式锁**

分布式锁算法Redlock

获取锁：

1. 客户端获取当前时间

2. 客户端**依次**（可以避免死锁）向N个Redis实例（都是master）执行加锁操作 为防止某个实例不可用 需要设置命令的超时时间 超时了就请求下一个实例

3. 客户端完成了和所有Redis实例的加锁操作 客户端就要计算加锁过程的总耗时 

   加锁成功需要满足的条件：

   - 从超过半数以上的Redis实例上获取了锁
   - 加锁时间 < 锁的有效时间

   成功后锁的有效时间需要减去加锁花费的时间

释放锁：依次在N个实例上执行lua脚本





## 事务机制：Redis能实现ACID属性吗



**Redis如何实现事务**

multi + exec(建议配合pipeline使用)

1. multi 显示表示一个事务的开启
2. 开启后 后续的命令发送到服务器端 并不会立即执行 而是会暂存在命令队列中
3. exec 命令表示提交事务 告诉服务端执行队列中的命令



**原子性**

- exec执行前 客户端发送的操作命令在入队是判断出来有错误(语法错误或命令不存在) 则exec时不会执行该事务 服务端会帮你执行discard

- exec执行前 入队时没有发现错误 但是无法执行  如命令和操作的数据类型不匹配 exec仍会执行事务中的其他命令 无法保证原子性（没有undo log 无法回滚）

  类似回滚的功能：discard 主动放弃事务执行 将暂存的命令队列清空

- exec执行过程中 Redis宕机 如果开启了AOF日志 可以用redis-check-aof 将AOF文件中未执行完的事务操作去掉 这样通过AOF恢复实例的时候 整个事务的修改都不会生效



**一致性**

> 没有包含非法或者无效的错误数据，从这个角度来理解一致性的话，只要没有执行错误的命令，那么就保证了一致性
>
> 从数据库层面，事物执行前后能够保证数据符合你设置的约束（如外键约束，唯一性约束，check约束等

无论什么情况下 错误的命令都不会执行成功



**隔离性**

watch命令：事务开启前监控key的变化 如果在exec的时候 发现key值有变化 则discard事务

- 并发操作在exec执行前 需要配合watch

  ![img](https://static001.geekbang.org/resource/image/4f/73/4f8589410f77df16311dd29131676373.jpg?wh=3000*1921)

- 并发操作在exec执行后 因为是单线程执行数据读写操作 无需watch



**持久性**

无法保证 不管是RDB还是 AOF: no everysec always

> AOF always
>
> “其实我们每次执行客户端命令的时候操作并没有写到aof文件中，只是写到了aof_buf内存当中，当进行下一个事件循环(event_loop)的时候执行beforeSleep之时，才会去fsync到disk中。”





## Redis主从同步与切换中的坑

主从集群好处：

- 让从库服务读请求 分担主库压力
- 主库故障时 进行主从切换 提高服务可靠性



**主从数据不一致**

> 客户端从从库中读取到的值与主库中的最新值不一致

造成原因：

主从库的命令复制是异步进行的，主库收到客户端的写命令后 在本地执行完就把结果返回给客户端了 同时会把写命令发送给从库

从库滞后执行同步命令的场景：

- 主从库间的网络传输延迟大
- 从库收到主库的命令时 正在处理其他复杂度较高的命令（如O(n)的集合操作命令）

解决方法：

- 硬件：

  保证主从库之间的网络连接状况良好 避免把主从库部署在不同的机房 避免把网络通信密集的应用与Redis主从库部署在一起（会占用网络带宽）

- 软件：

  `info replication` 查看主库接收写命令的进度信息(master_repl_offset)与从库复制写命令的进度信息(slave_repl_offset)  监控其差值 如果差值过大 则不让客户端从这个从库读取数据 差值变小后再放回来



**读取过期数据**

> 客户端会从从库中读取到本应过期的数据

造成原因：

过期数据的删除策略（3.2版本之前从库会读到 3.2之后返回空值）

- 惰性删除（跟guava cache有点像）

  当一个数据的过期时间到期后 不会立即删除数据 而是等待再有请求来访问时 当检查到数据已经过期了 就删除这个数据（主库上是这样 从库上过期了直接返回空值）

  好处：节省删除操作占用的CPU

  坏处：大量过期数据留存在内存中（又是trade-off）

- 定期删除

  每个100ms 随机选出一批数据 将其中过期的数据删除

2. 设置过期时间的命令 使用了expire或pexpire

   数据存活时间会受到命令执行时间的影响 从命令开始执行的时候往后推

   建议使用expireat/pexpireat 将数据的过期时间设置为具体的时间点 避免在从库读到过期数据



**不合理配置项导致服务挂掉**

- protected-more

  决定哨兵示例是否可以被其他服务器访问 为yes时只能在本地服务器方位

  所以需要设置为no 并且绑定其他哨兵实例的IP

  ```json
  protected-mode no
  bind 192.168.10.3 192.168.10.4 192.168.10.5
  ```

- cluster-node-timeout

  在Redis Cluster集群中为每个实例配置了一主一从模式的时候 当主从切换的时候 超过实例的心跳时间时  就会被集群判定为异常 超过半数以上实例异常 集群不可用 所以需要将这个超时时间调大



## 脑裂：一次奇怪的数据丢失

> 脑裂：主从集群中 同一个时间段内 出现了两个主节点 都能接收写请求

**为什么会发生脑裂**

场景：客户端数据丢失

1. 排查是否是主从同步出了问题 主库中的数据还未同步到从库 就宕机了 从库升级为主库后 这部分数据就丢失了

   此时master_repl_offset > slave_repl_offset

2. 排查客户端操作日志 发现脑裂现象

   在主从切换后 如果原主库只是假故障 恢复之后某个客户端仍然和原主库进行通信 原主库降级为从库后 需要清空本地数据 加载新主库的RDB 从而丢失那部分数据

![img](https://static001.geekbang.org/resource/image/95/66/959240fa59c2bb9f5ddb7df4b318af66.jpg?wh=2671*1872)



**如何应对脑裂问题**

让假故障的原主库无法接收客户端请求即可 假故障期间min-slaves-to-write 小于设定值 或min-slaves-max-lag 大于设定值 则主库无法接收客户端请求 新写的数据只会写到新主库上去 

- min-slaves-to-write：主库能进行数据同步的最小从库数量
- min-slaves-max-lag：主从同步时 从库给主库发ACK消息的最大延迟



假故障原因：短时间内无法处理信息 但是不是宕机

- 内因：自身操作阻塞 如处理bigkey或内存swap
- 外因：同服务器上其他进程临时占用过多资源



## 23-33讲课后思考题答案

**Redis只读缓存和读写缓存的区别**

修改数据时 只读缓存是修改数据库成功后 将key从缓存中删除 而读写缓存是既修改数据库又修改缓存 

![img](https://static001.geekbang.org/resource/image/84/51/84ed48ebccd3443f29cba150b5c1a951.jpg?wh=2822*1556)



**服务熔断 服务降级 请求限流可以解决缓存穿透问题吗**

缓存穿透本质上是请求了缓存和数据库中没有的数据

这3个方法本质上是为了解决Redis没有起到缓存层的作用的问题



**使用了LFU淘汰策略之后 缓存还会被污染吗**

会 极端情况下 某个key短时间内被高频访问 同时计数器衰减地很慢 这个数据还是会长时间滞留在内存中 造成污染



**在执行事务的过程中 Redis实例发生故障 采用的RDB机制 此时能保证原子性吗**

分情况：

1. 事务执行到一半时 Redis宕机 此时可以保证
2. 事务执行完 下一个RDB快照生成了 此时可以保证
3. 事务执行完 但是下一个RDB快照没有生成 此时无法保证



## Redis支持秒杀场景的关键技术和实践

**秒杀场景的负载特征对支撑系统的要求**

- 瞬时并发访问量非常高

  Redis的能处理的并发量是万级 MySQL能处理的并发量是千级

  所以有大量并发请求涌入秒杀系统时 可以用Redis拦截大部分请求 避免请求直接打到数据库 

- 读多写少 读操作是简单的查询操作

  先查询库存（读） 当还有库存的时候 再进行库存扣减和下单（写操作）



**Redis可以在秒杀场景的哪些环节发挥作用**

- 秒杀活动前：用户不断刷新商品详情页 导致详情页的请求量剧增 

  可以将详情页的页面元素静态化（之前有可能是动态请求接口渲染的） 然后使用CDN和浏览器将静态化的元素缓存起来

- 秒杀活动开始：用户点击秒杀按钮 库存查验 库存扣减 订单处理

  并发压力最大的是库存查验 库存扣减也需要放在Redis中 且两个操作需要保证原子性

  库存扣减放在数据库中的弊端：

  - 带来额外的开销 库存量在Redis中保存 同时最新值在数据库中更新 更新完需要同步回Redis
  - 下单量超过实际库存量 出现超售 数据库中的处理速度慢 大量的库存查验还是读取的Redis中的旧值 请求MySQL 如果不加悲观锁或者乐观锁的话 会出现超售的情况

- 秒杀活动结束后：部分用户刷新详情页 用户退单 成功下单的用户查询订单 但是用户请求量已经下降很多了

![img](https://static001.geekbang.org/resource/image/7c/1b/7c3e5def912d7c8c45bca00f955d751b.jpg?wh=2176*1582)



**Redis的哪些方法可以支撑秒杀场景**

秒杀场景对Redis的要求：

- 支持高并发：Redis本身就支持高并发 同时如果有多种商品 也可以把不同商品放到不同的切片集群中 同时提前算好商品的key放到的slot位置 分配路由表
- 保证库存查验和库存扣减的原子性：
  - 原子操作
  - 分布式锁



**基于原子操作支撑秒杀场景**

```json
key: itemID
// total 总库存量 ordered已秒杀量
value: {total: N, ordered: M}
```

```lua
# 因为库存查验和库存扣减(扣减多个) 无法通过单命令完成 所以通过lua脚本
#获取商品库存信息            
local counts = redis.call("HMGET", KEYS[1], "total", "ordered");
#将总库存转换为数值
local total = tonumber(counts[1])
#将已被秒杀的库存转换为数值
local ordered = tonumber(counts[2])  
#如果当前请求的库存量加上已被秒杀的库存量仍然小于总库存量，就可以更新库存         
if ordered + k <= total then
    #更新已秒杀的库存量
    redis.call("HINCRBY",KEYS[1],"ordered",k)                              return k;  
end               
return 0
```



**基于分布式锁来支撑秒杀场景**

只有拿到分布式锁的客户端才能进行库存查验和扣减

```c

//使用商品ID作为key
key = itemID
//使用客户端唯一标识作为value
val = clientUniqueID
//申请分布式锁，Timeout是超时时间
lock =acquireLock(key, val, Timeout)
//当拿到锁后，才能进行库存查验和扣减
if(lock == True) {
   //库存查验和扣减
   availStock = DECR(key, k)
   //库存已经扣减完了，释放锁，返回秒杀失败
   if (availStock < 0) {
      releaseLock(key, val)
      return error
   }
   //库存扣减成功，释放锁
   else{
     releaseLock(key, val)
     //订单处理
   }
}
//没有拿到锁，直接返回
else
   return
```



秒杀系统的其他环节：

- 前端静态页面的设计：能静态化的页面元素都尽量静态化
- 请求拦截与流量控制：避免恶意请求攻击  在接入层限流 控制进入秒杀系统的请求数量 
- 避免给Redis库存信息设置过期时间
- 数据库订单异常处理 添加订单重试功能



## 集群方案选择：Codis vs Redis Cluster

**Codis整体架构与基本流程**

组件：

- codis server：redis实例  内置了数据迁移功能
- codis proxy：接收并转发客户端请求到codis server
- zookeeper集群：保存元数据 如数据位置信息与codis proxy信息
- codis dashboard codis fe：集群管理工具

![img](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202212160734710.jpg)



影响切片集群使用效果的技术因素：

- 数据分布
- 集群扩容与数据迁移
- 客户端兼容性
- 可靠性保证



**数据如何在集群中分布**

1. 分为1024个slot 建立slot与codis server的映射(数据路由表 在dashboard配置号 发送到proxy 保存在zookeeper)
2. 客户端读写数据时 使用CRC32 计算key的哈希值 并对1024取模 根据映射找到server

与Redis Cluster区别：

- Codis：

  中心化 路由表存储在zookeeper 路由表变化时 可以直接把新的路由表发送给proxy

- Redis Cluster：

  去中心化 每个实例上都保存一份 路由表变化时 需要广播到所有实例上



**集群扩容和数据迁移怎么做**

集群扩容：

- 增加codis server
- 增加codis proxy



**增加codis server**

1. 启动新的codis server 将其加到集群

2. 将部分数据迁移到新的server

   迁移模式：

   - 同步迁移

     迁移slot时 会阻塞住源server 导致其无法处理新的请求

   - 异步迁移（采用的模式）

     源server将数据发送给目的server后 就可以处理其他请求了 当目的server接收到数据 反序列化 并保存到本地后 会给源server发送一个ACK消息 表明迁移完成 源server此时才会把数据删除 迁移过程中数据会被设置为只读 所以不会导致数据不一致的问题

     对于bigkey中的每个元素都单独用一条指令进行迁移 化整为零 元素迁移到目的server后会设置过期时间 当整个都迁移完成会将过期时间删除 如果迁移过程中发送故障 则到达目的server的部分bigkey元素就会过期删除

     同时也允许每次迁移多个key



**增加codis proxy**

1. 启动proxy
2. 在dashboard上添加 新的proxy信息保存到zookeeper
3. 客户端从zookeeper中读取最新的proxy访问列表



**集群客户端需要重新开发吗**

不需要 请求转发和数据迁移都由内置组件完成



**集群可靠性怎么保证**

- Codis Server

  主从集群 + 哨兵

  ![](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202212160842130.jpg)

- codis proxy和zookeeper

  zookeeper集群只有半数以上的实例可以正常工作就可以提供服务 proxy的路由表信息都保存在zookeeper中 故障重启时再通过codis dashboard从zookeeper集群上获取路由表即可

  ![img](https://static001.geekbang.org/resource/image/8f/b8/8fec8c2f76e32647d055ae6ed8cfbab8.jpg?wh=2880*1166)





## 数据分布优化：如何应对数据倾斜

数据倾斜类型：

- 数据量倾斜：某个实例上的数据特别多
- 数据访问倾斜：每个实例数据层相差不大 但是某个实例上的数据是热点数据



**数据量倾斜的成因和应对方法**

成因：

- bigkey：

  value占的空间很大 或者集合类型的value中的元素很多

  应对方法：

  避免把过多数据放在同一个键值对中 集合类型的bigkey可以尝试将其拆分为多个小的集合类型 分散保存在不停实例上

- Slot分配不均衡

  避免一个实例分配了过多的slot 如果有可以用迁移命令迁移

- Hash Tag

  key中的花括号 {}  客户端在计算key的CRC16值时 只会对{}中的内容进行计算 key不同 {} tag里面内容相同的key会被映射到同一个slot中

  使用hash tag的原因 是因为redis不支持跨实例的事务操作和范围查询

  应对方式：

  不使用hash tag 事务操作和范围查询放在客户端执行



**数据访问倾斜的成因和应对方法**

成因：实例上存在热点数据

解决方法：

- 如果是只读数据 可以采用多副本的方法 给key加不同的前缀
- 如果是读写数据 只能给实例增加资源





## 通信开销：限制Redis Cluster规模的关键因素

Redis Cluster规模上限：1000个实例

有上限的原因：实例间的通信开销会随着实例规模增加而增大



**实例通信方法和对集群规模的影响**

Gossip协议：

同步新节点加入 节点故障 Slot变更等事件

- ping：

  每个实例会按照一定的频率  挑选出集群中的一些实例 给实例发送ping消息（包含 自身状态信息 部分其他实例状态信息 slot映射表）

- pong：

  收到ping后 也给ping的实例 发送自己知道的信息

![img](https://daxiao-img.oss-cn-beijing.aliyuncs.com/img/202212181607655.jpg)



**通信开销**：

- 消息大小

  ```c
  // 一个实例的状态啊信息的大小 104B
  typedef struct {
      char nodename[CLUSTER_NAMELEN];  //40字节
      uint32_t ping_sent; //4字节
      uint32_t pong_received; //4字节
      char ip[NET_IP_STR_LEN]; //46字节
      uint16_t port;  //2字节
      uint16_t cport;  //2字节
      uint16_t flags;  //2字节
      uint32_t notused1; //4字节
  } clusterMsgDataGossip;
  ```

  自身状态信息 + 集群十分之一实例的状态信息

  1000个实例 就差不多是10K

  Slot映射表：16384bit -> 2K

  所以一个PING消息在1000个实例下的大小为12K

- 通信频率

  - 每秒从本地实例列表中随机选出5个实例 再从这5个实例中找出最久没有通信的实例进行PING
  - 100ms一次 扫描本地实例列表 如果发现实例最近一次接收PONG的时间 大于cluster-node-time/2 就给该实例发送PING消息 

  

**如何降低实例间的通信开销**

通信消息的大小不能减小

只能降低传输频率 可以适当调大cluster-node-time 15s -> 25s





